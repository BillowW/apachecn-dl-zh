# 使用 TensorFlow 的关系和匹配网络

在上一章中，我们了解了原型网络，以及如何将原型网络的变体（例如高斯原型网络和半原型网络）用于一次性学习。 我们已经看到原型网络如何利用嵌入来执行分类任务。

在本章中，我们将学习关系网络和匹配网络。 首先，我们将了解什么是关系网络以及如何在单次，几次和零次学习设置中使用它，然后，我们将学习如何使用 TensorFlow 建立关系网络。 在本章的后面，我们将学习匹配网络以及如何在几次学习中使用它们。 我们还将看到在匹配网络中使用的不同类型的嵌入函数。 在本章的最后，我们将看到如何在 Tensorflow 中构建匹配的网络。

在本章中，我们将学习以下内容：

*   关系网络
*   单次，少次和零次设置的关系网络
*   使用 TensorFlow 建立关系网络
*   匹配网络
*   匹配网络的嵌入函数
*   匹配网络的架构
*   TensorFlow 中的匹配网络

# 关系网络

现在，我们将看到另一种有趣的单次学习算法，称为关系网络。 它是最简单，最有效的单发学习算法之一。 我们将探讨在单发，少发和零发学习设置中如何使用关系网络。

# 一次性学习中的关系网络

关系网络由两个重要功能组成：以![](img/f9704c53-02aa-4996-8ec8-bb5e3510e5df.png)表示的嵌入函数和以![](img/619a6f8b-c314-49a6-8846-9625d234030e.png)表示的关系功能。 嵌入函数用于从输入中提取特征。 如果输入是图像，则可以使用卷积网络作为嵌入函数，这将为我们提供图像的特征向量/嵌入。 如果我们的输入是文本，那么我们可以使用 LSTM 网络获取文本的嵌入。

众所周知，在一次学习中，每个班级只有一个示例。 例如，假设我们的支持集包含三个类，每个类一个示例。 如下图所示，我们有一个包含三个类别的支持集，`{Lion, Eleph, Dog}`：

![](img/030c5e04-cf05-4394-baad-3c43bc5f77fa.png)

假设我们有一个查询图像![](img/a69c2a9f-d4fb-413c-8047-bc6fc139a536.png)，如下图所示，我们希望预测该查询图像的类：

![](img/87783c42-eb05-4a41-aecf-ab7bdfa56cb5.png)

首先，我们从支持集中获取每个图像![](img/1078376b-32e4-4899-9606-74eeb970e0f3.png)，并将其传递给嵌入函数![](img/61752837-2d9c-4f9a-a604-d86fd605e9bd.png)，以提取特征。 由于我们的支持集包含图像，因此我们可以使用卷积网络作为我们的嵌入函数来学习嵌入。 嵌入函数将为我们提供支持集中每个数据点的特征向量。 类似地，我们将把查询图像![](img/b838f304-5607-4be2-b374-90474eb18833.png)传递给嵌入函数![](img/ad6baf80-8d13-4f1d-9ba2-9a90a9cb8b4d.png)来学习其嵌入。

因此，一旦有了支持集![](img/61752837-2d9c-4f9a-a604-d86fd605e9bd.png)和查询集![](img/ad6baf80-8d13-4f1d-9ba2-9a90a9cb8b4d.png)的特征向量，就可以使用运算符![](img/be540eca-a2f6-40f1-ae07-28c2fbfb136c.png)组合它们。 ![](img/b0dd8d2d-ae04-40ba-9ded-b799bc692200.png)可以是任何组合运算符； 我们使用串联作为运算符，以合并支持和查询集的特征向量，即![](img/c7f2ec55-cb79-45f5-b62c-70f8e77e3cf4.png)。

如下图所示，我们将合并支持集![](img/61752837-2d9c-4f9a-a604-d86fd605e9bd.png)和查询集![](img/ad6baf80-8d13-4f1d-9ba2-9a90a9cb8b4d.png)的特征向量。 但是这样的组合有什么用呢？ 这将帮助我们理解支持集中图像的特征向量与查询图像的特征向量之间的关系。 在我们的示例中，它将帮助我们理解狮子，大象和狗的图像的特征向量与查询图像的特征向量之间的关系：

![](img/3cee7837-9e4c-469d-931b-c64c706de99b.png)

但是我们如何衡量这种关联性呢？ 这就是为什么我们使用关系函数![](img/e53a6f8b-0846-43ff-b600-67b405404468.png)的原因。 我们将这些组合的特征向量传递给关系函数，该函数将生成从 0 到 1 的关系得分，代表支持集![](img/f3df92cc-3b9f-4296-97ac-a4caecbebf5d.png)中的样本与查询集![](img/2b82f087-237b-4c95-8083-de38d0232742.png)中的样本之间的相似性。

以下等式说明了我们如何计算关系网络中的关系得分：

![](img/4d723f33-f865-4a5b-b387-b29268e8a070.png)

在该等式中，![](img/b6b80ad2-1186-4cca-8391-cf256c231a44.png)表示表示在支持集中的每个类别和查询图像之间的相似性的关系分数。 由于我们在支持集中有 3 个类别，在查询集中有 1 个图像，因此我们将获得 3 个分数，表明支持集中的所有 3 个类别与查询图像的相似程度。

下图显示了在一次学习设置中关系网络的整体表示：

![](img/5f7d57ed-3814-4506-a8d2-da060f75ef00.png)

# 一次性学习中的关系网络

我们已经看到了如何拍摄属于支持集中每个类别的单个图像，并在关系网络的单次学习设置中将它们与查询集中图像的关系进行比较。 但是，在几次学习设置中，每个班级将有多个数据点。 我们如何使用嵌入函数在此处学习特征表示？

假设我们有一个支持集，每个类包含一个以上的图像，如下图所示：

![](img/6b08140c-a4cc-4172-9a4e-b59d58ba4fd9.png)

在这种情况下，我们将学习支持集中每个点的嵌入，并对属于每个类的所有数据点进行元素逐级添加。 因此，我们将为每个类都有嵌入，这是该类中所有数据点的逐元素求和的嵌入：

![](img/fdf9f217-9695-462c-be8b-e6efdf3832f2.png)

我们可以像往常一样使用嵌入函数来提取查询图像的特征向量。 接下来，我们使用连接运算符![](img/6a904b6e-8b8d-4991-8097-5d179786d4ed.png)组合支持和查询集的特征向量。 我们执行级联，然后将级联的特征向量输入到关系函数并获得关系得分，该关系得分表示支持集和查询集中每个类之间的相似性。

下图显示了关系网络在几次学习设置中的整体表示：

![](img/61827802-face-4bba-9b33-1a79c5d2deaf.png)

# 零镜头学习中的关系网络

既然我们已经了解了如何在单发和少发学习任务中使用关系网络，我们将看到如何在零发学习设置中使用关系网络，在这种情况下，每个类别下都没有任何数据点。 但是，在零射击学习中，我们将具有元信息，该元信息是有关每个类的属性的信息，并将被编码到语义向量![](img/9fadf51c-0b77-4e48-908e-5fcc1eb85df1.png)中，其中下标 c 表示类。

我们没有使用单个嵌入函数来学习支持和查询集的嵌入，而是分别使用了两个不同的嵌入函数![](img/86aecac3-d041-4699-8b22-138ea9800f42.png)和![](img/60564a32-a8fc-477b-ab73-2f4cd0406154.png)。 首先，我们将使用![](img/86aecac3-d041-4699-8b22-138ea9800f42.png)学习语义向量![](img/9eddfb58-cbd7-4956-9c7f-ab8cee401f2f.png)的嵌入，并使用![](img/c3eb60fd-af00-4e27-a9e6-099a893e3da2.png)学习查询集![](img/2ef03adb-44aa-47b0-a212-270cef406cef.png)的嵌入。 现在，我们将使用串联操作![](img/7f57c532-4859-4c27-be61-2e5f592489e9.png)来串联这些嵌入：

![](img/d30a1e84-e7f0-486f-ba63-07bd509357ca.png)

然后，我们将此结果馈入关联函数并计算关联分数，如下所示：

![](img/79ed7a01-bf4f-447a-b3a6-adc736e08ec7.png)

# 损失函数

关系网络的损失函数是什么？ 我们将使用**均方误差**（**MSE**）作为损失函数。 尽管这是一个分类问题，而 MSE 并不是分类问题的标准度量，但关系网络的作者表示，由于我们正在预测关系得分，因此可以将其视为回归问题。 尽管如此，对于基本事实，我们只能自动生成`{0, 1}`目标。

因此，我们的损失函数可以表示为：

![](img/aa46d3e5-b4a1-4780-8b9f-f095b87b18f5.png)

其中![](img/8c4472bf-e13e-4bd0-993f-f61692d3fc5a.png)分别是我们嵌入函数![](img/2cacbc1d-22e1-4224-8bf5-9e4ff2408d9e.png)和关联函数![](img/ce50f7c9-71d5-4379-a9a8-886386051514.png)的参数。

# 使用 TensorFlow 建立关系网络

关系函数非常简单，对吧？ 通过在 TensorFlow 中实现一个关系网络，我们将更好地理解关系网络。

[您还可以在此处查看 Jupyter Notebook 中可用的代码并进行解释](https://github.com/sudharsan13296/Hands-On-Meta-Learning-With-Python/blob/master/04.%20Relation%20and%20Matching%20Networks%20Using%20Tensorflow/4.5%20Building%20Relation%20Network%20Using%20Tensorflow.ipynb)。

首先，我们导入所有必需的库：

```py
import tensorflow as tf
import numpy as np
```

我们将随机生成数据点。 假设我们的数据集中有两个类； 我们将为这些类别中的每一个随机生成约 1,000 个数据点：

```py
classA = np.random.rand(1000,18)
ClassB = np.random.rand(1000,18)
```

我们通过结合以下两个类来创建数据集：

```py
data = np.vstack([classA, ClassB])
```

现在，我们设置标签； 我们为`classA`分配`1`标签，为`classB`分配`0`标签：

```py
label = np.vstack([np.ones((len(classA),1)),np.zeros((len(ClassB),1))])
```

因此，我们的数据集将有 2,000 条记录：

```py
data.shape
(2000, 18)
```

现在，我们将为支持和查询集定义占位符：

```py
xi = tf.placeholder(tf.float32, [None, 9])
xj = tf.placeholder(tf.float32, [None, 9])
```

定义`y`标签的占位符，如下所示：

```py
y = tf.placeholder(tf.float32, [None, 1]) 
```

现在，我们将定义我们的嵌入函数，该功能将学习支持和查询集的嵌入。 我们将使用普通的前馈网络作为嵌入函数：

```py
def embedding_function(x):

    weights = tf.Variable(tf.truncated_normal([9,1]))
    bias = tf.Variable(tf.truncated_normal([1]))

    a = (tf.nn.xw_plus_b(x,weights,bias))
    embeddings = tf.nn.relu(a)

    return embeddings
```

我们计算支持集的嵌入量：

```py
f_xi = embedding_function(xi)
```

我们计算查询集的嵌入量：

```py
f_xj = embedding_function(xj)
```

现在我们已经计算了嵌入并有了特征向量，我们将支持集和查询集特征向量结合起来：

```py
Z = tf.concat([f_xi,f_xj],axis=1)
```

我们将关系函数定义为具有 ReLU 激活的三层神经网络：

```py
def relation_function(x):
    w1 = tf.Variable(tf.truncated_normal([2,3]))
    b1 = tf.Variable(tf.truncated_normal([3]))

    w2 = tf.Variable(tf.truncated_normal([3,5]))
    b2 = tf.Variable(tf.truncated_normal([5]))

    w3 = tf.Variable(tf.truncated_normal([5,1]))
    b3 = tf.Variable(tf.truncated_normal([1]))

    #layer1
    z1 = (tf.nn.xw_plus_b(x,w1,b1))
    a1 = tf.nn.relu(z1)

    #layer2
    z2 = tf.nn.xw_plus_b(a1,w2,b2)
    a2 = tf.nn.relu(z2)

    #layer3
    z3 = tf.nn.xw_plus_b(z2,w3,b3)

    #output
    y = tf.nn.sigmoid(z3)

    return y
```

现在，我们将支持集和查询集的串联特征向量传递给关系函数，并获得关系得分：

```py
relation_scores = relation_function(Z)
```

我们将`loss_function`计算为 MSE，即`relation_scores`与实际`y`值之间的`squared_difference`：

```py
loss_function = tf.reduce_mean(tf.squared_difference(relation_scores,y))
```

我们可以使用`AdamOptimizer`将损失降到最低：

```py
optimizer = tf.train.AdamOptimizer(0.1)
train = optimizer.minimize(loss_function)
```

现在，让我们开始 TensorFlow 会话：

```py
sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())
```

现在，我们随机抽取支持集`xi`和查询集`xj`的数据点，并训练网络：

```py
for episode in range(1000):
    _, loss_value = sess.run([train, loss_function], 
                             feed_dict={xi:data[:,0:9]+np.random.randn(*np.shape(data[:,0:9]))*0.05,
                                        xj:data[:,9:]+np.random.randn(*np.shape(data[:,9:]))*0.05,
                                        y:label})
    if episode % 100 == 0:
        print("Episode {}: loss {:.3f} ".format(episode, loss_value))
```

我们可以看到如下输出：

```py
Episode 0: loss 0.495 
Episode 100: loss 0.250 
Episode 200: loss 0.250 
Episode 300: loss 0.250 
Episode 400: loss 0.250 
Episode 500: loss 0.250 
Episode 600: loss 0.250 
Episode 700: loss 0.250 
Episode 800: loss 0.250 
Episode 900: loss 0.250 
```

# 匹配网络

匹配网络是 Google 的 DeepMind 团队发布的另一种简单高效的一次性学习算法。 它甚至可以为数据集中未观察到的类生成标签。

假设我们有一个支持集`S`，其中包含`K`示例作为![](img/7de9a941-8801-4363-ac4e-f9ae0c13a716.png)。 给定查询点（一个新的看不见的示例）![](img/9385cadf-442c-46da-b498-a613372648fc.png)时，匹配网络通过将其与支持集进行比较来预测![](img/f3d60daa-f8f6-4be2-b246-984bea7eb4e2.png)的类别。

我们可以将其定义为![](img/d9ccc420-177f-4115-a8e1-7d65897c692a.png)，其中![](img/318eca5d-7d4f-4d93-828d-f0444520e952.png)是参数化神经网络，![](img/ed201434-5cfd-4a61-bdb5-ec02a9901af6.png)是查询点的预测类，![](img/d3aaea41-a25b-424c-a540-b245800d4613.png)和![](img/3755650d-21aa-4377-b997-5d69dbfa2ec9.png)是支持集。 ![](img/d9ccc420-177f-4115-a8e1-7d65897c692a.png)将返回![](img/d31b51a8-0ee8-494b-b91f-81873448c0c4.png)属于数据集中每个类别的概率。 然后，我们选择![](img/2af6be12-a8d4-4452-8df2-a5fec999a883.png)的类别作为可能性最高的类别。 但是，这到底如何工作？ 如何计算此概率？ 让我们现在看看。

查询点![](img/00d00799-95bf-4a1f-9eeb-2a2036114945.png)的输出![](img/ed2b81c9-3cb5-4e32-9649-63bbac61f5ca.png)可以预测如下：

![](img/dae716c0-28ab-4ad4-b488-6e5b4915aead.png)

让我们破译这个方程式。 ![](img/1e6b17c3-d908-46c6-93bc-c0bb7f15fed2.png)和![](img/6d55955b-5e84-41f6-babe-707bb2368af4.png)是支持集的输入和标签。 ![](img/c99092bd-ca8f-4410-8c98-f3763277f841.png)是查询输入，即我们要预测标签的输入。 ![](img/cc8e4200-b6cf-48af-84c3-1b6037fe5fdc.png)是![](img/fc4e3de9-a4be-4fdc-bacc-8753f9ab8b5a.png)和![](img/baceddf4-d3f2-476d-a311-7c2c26b6abf9.png)之间的注意力机制。 但是，我们该如何进行关注呢？ 在这里，我们使用一种简单的注意机制，即![](img/cf2bb81f-0cd2-41d9-928b-a5f04b45407c.png)和![](img/63c54228-3b78-4be1-9900-dcaea5aa67d8.png)之间（即![](img/fdf5cddd-d8de-4c1c-bc4a-06305163cedd.png)）的余弦距离上的 softmax 函数。

我们无法直接计算原始输入![](img/6e5b33d3-a4ef-4260-a63f-ba22cfb11ced.png)和![](img/dbd64ebc-d0a5-479d-aa27-697de19d518a.png)之间的余弦距离。 因此，首先，我们将学习它们的嵌入并计算嵌入之间的余弦距离。 我们使用两种不同的嵌入![](img/2e2245e2-5a20-4624-9c2a-0e581a88ab5d.png)和![](img/c794b2cc-cc8d-4917-ab1b-73f3134fc859.png)来分别学习查询输入![](img/f3f565ae-e636-4c48-b734-98717227b7b8.png)和支持集输入![](img/8a76ce9f-e0dc-4ea8-908e-0e1f0b4fe08f.png)的嵌入。 我们将在接下来的部分中详细了解![](img/3ca0ce74-1ec2-4adc-8763-e4764a603c8c.png)和![](img/b2e015e7-da70-4cb6-8e03-34c28cbc61ca.png)这两个嵌入函数。

因此，我们可以如下重写注意力方程：

![](img/a4a417ee-95be-4bff-99ac-da7c59223a30.png)

我们可以将前面的等式重写如下：

![](img/fd6aef37-dfe5-45f4-87a3-799cfd6e7c43.png)

因此，在计算注意力矩阵![](img/3d2735a2-46f8-4271-9ac1-b3b77cfa124f.png)之后，我们将注意力矩阵与支持集标签![](img/4eb03151-9951-4817-ae00-14c1d60f7229.png)相乘。 但是，如何将支持集标签与注意力矩阵相乘呢？ 首先，我们将支持集标签转换为一个热编码值，然后将它们与我们的注意力矩阵相乘，结果，我们获得了![](img/685d4fc8-b1ca-41aa-a68c-60b5378803bf.png)属于支持集中每个类的概率。 然后，我们应用 argmax 并选择![](img/5d76bf68-0779-4160-8162-35ee38491efe.png)作为具有最大概率值的那个。

您是否还不清楚匹配网络？ 看下图； 如您所见，我们的支持集中有 3 个类，即`{Lion, Eleph, Dog}`，还有一个新的查询图像![](img/6ed1cd67-f5a3-4556-a51f-293703675687.png)。 首先，将支持集提供给嵌入函数![](img/ea0c80de-3df9-49a0-8e3d-1ef56cf33ddd.png)，将查询图像提供给嵌入函数![](img/c4130722-4028-4ab3-819d-53b515d033d4.png)，然后学习它们的嵌入并计算它们之间的余弦距离； 然后，我们在这个余弦距离上施加 softmax 注意。 然后，将注意力矩阵与一键编码支持集标签相乘，得到概率，然后选择![](img/0c52e07c-051e-497e-85ef-db57678bb2b2.png)作为概率最高的那个。 如下图所示，查询集图像是一头大象，我们在索引 1 处的概率很高，因此我们将![](img/b9a8f80e-6f5f-4aee-a138-7b0d7af50c5c.png)的类别预测为 1（大象）：

![](img/92322e5f-a6ae-42d2-b3e4-6d4820b5dde8.png)

# 嵌入函数

我们了解到，我们使用两个嵌入函数![](img/722c4cab-8c9b-45aa-87b4-d2d171e23c57.png)和![](img/583e2b77-74e9-44aa-be8e-fa6b435d77c9.png)分别学习![](img/8298038a-6f99-42d3-af60-9278249d5b15.png)和![](img/aaf53641-2bb7-4441-a93c-e4599f57b944.png)的嵌入。 现在，我们将确切地看到这两个函数如何学习嵌入。

# 支持集嵌入函数（g）

我们使用嵌入函数![](img/d9b6aae8-0813-4ee5-9c07-aac1e3e1ba7d.png)来学习支持集的嵌入。 我们使用双向 LSTM 作为我们的嵌入函数![](img/6e84bc5e-d86c-4b7b-9b3b-dca3232ae883.png)。

我们可以如下定义嵌入函数![](img/71b0b590-c21a-4b97-9a91-3884d1a9ce57.png)：

```py
def g(X):

    #forward cell
    forward_cell = rnn.BasicLSTMCell(32)

    #backward cell
    backward_cell = rnn.BasicLSTMCell(32)

    #bidirectional LSTM
    outputs, state_forward, state_backward = rnn.static_bidirectional_rnn(forward_cell, backward_cell, X, dtype=tf.float32)

    return tf.add(tf.stack(X), tf.stack(outputs))
```

# 查询集嵌入函数（f）

我们使用嵌入函数![](img/4024d207-073d-4cc7-a80d-34668228b850.png)来学习查询点![](img/2387b169-8801-46a6-ac83-0cb1bcc89478.png)的嵌入。 我们使用 LSTM 作为编码功能。 与![](img/3fad2400-4c8f-46c6-aca8-d4dab0f04aa6.png)作为输入一起，我们还将传递支持集嵌入的嵌入`g(x)`，还将传递另一个名为`K`的参数，该参数定义了处理步骤的数量。 让我们逐步了解如何计算查询集嵌入。

首先，我们将初始化 LSTM 单元：

```py
cell = rnn.BasicLSTMCell(64)
previous_state = cell.zero_state(batch_size, tf.float32) 
```

然后，对于处理步骤数，我们执行以下操作：

```py
for step in xrange(K):
```

我们通过将查询集![](img/95026996-9724-475a-aa20-33d2cee09967.png)馈送到 LSTM 单元来计算其嵌入：

```py
     output, state = cell(XHat, previous_state) 
     h_k = tf.add(output, XHat)
```

现在，我们对支持集嵌入（即`g_embedings`）执行 softmax 注意。 它可以帮助我们避免不必要的元素：

```py
     content_based_attention = tf.nn.softmax(tf.multiply(previous_state[1], g_embedding)) 
     r_k = tf.reduce_sum(tf.multiply(content_based_attention, g_embedding), axis=0) 
```

我们更新`previous_state`，并在许多处理步骤`K`中重复这些步骤：

```py
    previous_state = rnn.LSTMStateTuple(state[0], tf.add(h_k, r_k))
```

计算`f_embeddings`的完整代码如下：

```py
def f(XHat, g_embedding, K):

    cell = rnn.BasicLSTMCell(64)
    previous_state = cell.zero_state(batch_size, tf.float32) 

    for step in xrange(K):
        output, state = cell(XHat, previous_state) 

        h_k = tf.add(output, XHat) 

        #Soft max attention
        content_based_attention = tf.nn.softmax(tf.multiply(previous_state[1], g_embedding)) 
        r_k = tf.reduce_sum(tf.multiply(content_based_attention, g_embedding), axis=0) 

        previous_state = rnn.LSTMStateTuple(state[0], tf.add(h_k, r_k))

    return output
```

# 匹配网络的架构

下图显示了匹配网络的整体流程，它与我们已经看到的图像不同。 您会注意到如何分别通过嵌入函数![](img/b9534b37-d67c-4462-ac1e-ea5770cfe006.png)和![](img/a3cb353d-cb0d-417f-a91b-2c2cd50269cb.png)计算支持集![](img/288cfb3c-fe6b-4d38-ae3b-c3458b6b3315.png)和查询集![](img/ae581489-e5b3-4cc6-920a-9b0532244e89.png)。 如您所见，嵌入函数`f`将查询集以及支持集嵌入作为输入：

![](img/5b6120e4-bb45-4acf-91ca-45fe924ae318.png)

# TensorFlow 中的匹配网络

现在，我们将逐步了解如何在 TensorFlow 中构建匹配的网络。 我们将在最后看到最终代码。

首先，我们导入库：

```py
import tensorflow as tf
slim = tf.contrib.slim
rnn = tf.contrib.rnn
```

现在，我们定义一个名为`Matching_network`的类，在其中定义我们的网络：

```py
class Matching_network():
```

我们定义`__init__`方法，在其中初始化所有变量：

```py

    def __init__(self, lr, n_way, k_shot, batch_size=32):

        #placeholder for support set
        self.support_set_image = tf.placeholder(tf.float32, [None, n_way * k_shot, 28, 28, 1])
        self.support_set_label = tf.placeholder(tf.int32, [None, n_way * k_shot, ])

        #placeholder for query set
        self.query_image = tf.placeholder(tf.float32, [None, 28, 28, 1])
        self.query_label = tf.placeholder(tf.int32, [None, ])
```

假设我们的支持集和查询集包含图片。 在将此原始图像提供给嵌入函数之前，首先，我们将使用卷积网络从图像中提取特征，然后将支持集和查询集的提取特征提供给`g`的嵌入函数 和`f`。

因此，我们将定义一个名为`image_encoder`的函数，该函数用于对图像中的特征进行编码。 我们使用具有最大池化操作的四层卷积网络作为图像编码器：

```py

   def image_encoder(self, image):

        with slim.arg_scope([slim.conv2d], num_outputs=64, kernel_size=3, normalizer_fn=slim.batch_norm):
            #conv1
            net = slim.conv2d(image)
            net = slim.max_pool2d(net, [2, 2])

            #conv2
            net = slim.conv2d(net)
            net = slim.max_pool2d(net, [2, 2])

            #conv3
            net = slim.conv2d(net)
            net = slim.max_pool2d(net, [2, 2])

            #conv4
            net = slim.conv2d(net)
            net = slim.max_pool2d(net, [2, 2])

        return tf.reshape(net, [-1, 1 * 1 * 64])
```

现在，我们定义嵌入函数； 我们已经看到在“嵌入函数”部分中如何定义嵌入函数`f`和`g`。 因此，我们可以直接定义它们如下：

```py
#embedding function for extracting support set embeddings
    def g(self, x_i):

        forward_cell = rnn.BasicLSTMCell(32)
        backward_cell = rnn.BasicLSTMCell(32)
        outputs, state_forward, state_backward = rnn.static_bidirectional_rnn(forward_cell, backward_cell, x_i, dtype=tf.float32)

        return tf.add(tf.stack(x_i), tf.stack(outputs))

    #embedding function for extracting query set embeddings
    def f(self, XHat, g_embedding):
        cell = rnn.BasicLSTMCell(64)
        prev_state = cell.zero_state(self.batch_size, tf.float32) 

        for step in xrange(self.processing_steps):
            output, state = cell(XHat, prev_state)

            h_k = tf.add(output, XHat) 

            content_based_attention = tf.nn.softmax(tf.multiply(prev_state[1], g_embedding)) 

            r_k = tf.reduce_sum(tf.multiply(content_based_attention, g_embedding), axis=0) 

            prev_state = rnn.LSTMStateTuple(state[0], tf.add(h_k, r_k))

        return output
```

现在，我们定义一个名为`cosine_similarity`的函数，用于学习支持集和查询集嵌入之间的余弦相似度：

```py
    def cosine_similarity(self, target, support_set):
        target_normed = target
        sup_similarity = []
        for i in tf.unstack(support_set):
            i_normed = tf.nn.l2_normalize(i, 1) 
            similarity = tf.matmul(tf.expand_dims(target_normed, 1), tf.expand_dims(i_normed, 2)) 
            sup_similarity.append(similarity)

        return tf.squeeze(tf.stack(sup_similarity, axis=1))
```

最后，我们使用一个名为`train`的函数来执行我们的训练操作-让我们逐步看一下：

```py
 def train(self, support_set_image, support_set_label, query_image):  
```

首先，我们使用图像编码器对支持集图像的功能进行编码：

```py
    support_set_image_encoded = [self.image_encoder(i) for i in tf.unstack(support_set_image, axis=1)]
```

然后，我们还将使用图像编码器对查询集图像的功能进行编码：

```py
    query_image_encoded = self.image_encoder(query_image)
```

接下来，我们将使用嵌入函数![](img/67537c53-9287-4705-a310-a3cfb3c01a98.png)了解支持集的嵌入：

```py
     g_embedding = self.g(support_set_image_encoded) 
```

同样，我们还将使用嵌入函数`f`了解查询集的嵌入内容：

```py
    f_embedding = self.f(query_image_encoded, g_embedding) 
```

现在，我们在这两个嵌入之间计算`cosine_similarity`：

```py
    embeddings_similarity = self.cosine_similarity(f_embedding, g_embedding) 
```

然后，我们对这种相似性进行 softmax 注意：

```py
    attention = tf.nn.softmax(embeddings_similarity)
```

我们通过将注意力矩阵乘以一热编码的支持集标签来预测查询集标签：

```py
    y_hat = tf.matmul(tf.expand_dims(attention, 1), tf.one_hot(support_set_label, self.n_way))
```

接下来，我们得到`probabilities`：

```py
    probabilities = tf.squeeze(y_hat)  
```

我们选择概率最高的索引作为查询图像的类别：

```py
    predictions = tf.argmax(self.logits, 1)
```

最后，我们定义损失函数； 我们使用 softmax 交叉熵作为我们的损失函数：

```py
    loss_function = tf.losses.sparse_softmax_cross_entropy(label, self.probabilities)
```

我们使用`AdamOptimizer`最小化损失函数：

```py
    tf.train.AdamOptimizer(self.lr).minimize(self.loss_op)
```

现在，我们将看到整个匹配网络的最终代码：

```py

class Matching_network():

    #initialize all the variables
    def __init__(self, lr, n_way, k_shot, batch_size=32):

        #placeholder for support set
        self.support_set_image = tf.placeholder(tf.float32, [None, n_way * k_shot, 28, 28, 1])
        self.support_set_label = tf.placeholder(tf.int32, [None, n_way * k_shot, ])

        #placeholder for query set
        self.query_image = tf.placeholder(tf.float32, [None, 28, 28, 1])
        self.query_label = tf.placeholder(tf.int32, [None, ])           

    #encoder function for extracting features from the image
    def image_encoder(self, image):

        with slim.arg_scope([slim.conv2d], num_outputs=64, kernel_size=3, normalizer_fn=slim.batch_norm):
            #conv1
            net = slim.conv2d(image)
            net = slim.max_pool2d(net, [2, 2])

            #conv2
            net = slim.conv2d(net)
            net = slim.max_pool2d(net, [2, 2])

            #conv3
            net = slim.conv2d(net)
            net = slim.max_pool2d(net, [2, 2])

            #conv4
            net = slim.conv2d(net)
            net = slim.max_pool2d(net, [2, 2])

        return tf.reshape(net, [-1, 1 * 1 * 64])

    #embedding function for extracting support set embeddings
    def g(self, x_i):

        forward_cell = rnn.BasicLSTMCell(32)
        backward_cell = rnn.BasicLSTMCell(32)
        outputs, state_forward, state_backward = rnn.static_bidirectional_rnn(forward_cell, backward_cell, x_i, dtype=tf.float32)

        return tf.add(tf.stack(x_i), tf.stack(outputs))

    #embedding function for extracting query set embeddings
    def f(self, XHat, g_embedding):
        cell = rnn.BasicLSTMCell(64)
        prev_state = cell.zero_state(self.batch_size, tf.float32) 

        for step in xrange(self.processing_steps):
            output, state = cell(XHat, prev_state)

            h_k = tf.add(output, XHat) 

            content_based_attention = tf.nn.softmax(tf.multiply(prev_state[1], g_embedding)) 

            r_k = tf.reduce_sum(tf.multiply(content_based_attention, g_embedding), axis=0) 

            prev_state = rnn.LSTMStateTuple(state[0], tf.add(h_k, r_k))

        return output

    #cosine similarity function for calculating cosine similarity between support set and query set embeddings
    def cosine_similarity(self, target, support_set):
        target_normed = target
        sup_similarity = []
        for i in tf.unstack(support_set):
            i_normed = tf.nn.l2_normalize(i, 1) 
            similarity = tf.matmul(tf.expand_dims(target_normed, 1), tf.expand_dims(i_normed, 2)) 
            sup_similarity.append(similarity)

        return tf.squeeze(tf.stack(sup_similarity, axis=1)) 

    def train(self, support_set_image, support_set_label, query_image): 

        #encode the features of query set images using our image encoder
        query_image_encoded = self.image_encoder(query_image) 

        #encode the features of support set images using our image encoder
        support_set_image_encoded = [self.image_encoder(i) for i in tf.unstack(support_set_image, axis=1)]

        #generate support set embeddings using our embedding function g
        g_embedding = self.g(support_set_image_encoded) 

        #generate query set embeddings using our embedding function f
        f_embedding = self.f(query_image_encoded, g_embedding) 

        #calculate the cosine similarity between both of these embeddings
        embeddings_similarity = self.cosine_similarity(f_embedding, g_embedding) 

        #perform attention over the embedding similarity
        attention = tf.nn.softmax(embeddings_similarity)

        #now predict query set label by multiplying attention matrix with one hot encoded support set labels
        y_hat = tf.matmul(tf.expand_dims(attention, 1), tf.one_hot(support_set_label, self.n_way))

        #get the probabilities 
        probabilities = tf.squeeze(y_hat) 

        #select the index which has the highest probability as a class of query image
        predictions = tf.argmax(self.probabilities, 1)

        #we use softmax cross entropy loss as our loss function
        loss_function = tf.losses.sparse_softmax_cross_entropy(label, self.probabilities)

        #we minimize the loss using adam optimizer
        tf.train.AdamOptimizer(self.lr).minimize(self.loss_op)
```

# 概要

在本章中，我们学习了在几次学习中如何使用匹配网络和关系网络。 我们看到了一个关系网络如何学习支持和查询集的嵌入，并将这些嵌入进行组合并将其馈送到关系函数以计算关系得分。 我们还看到了匹配的网络如何使用两种不同的嵌入函数来学习我们的支持集和查询集的嵌入，以及它如何预测查询集的类。

在下一章中，我们将通过存储和检索内存中的信息来学习神经图灵机和内存增强型神经网络的工作方式。

# 问题

1.  关系网络中使用的功能有哪些不同类型？
2.  关系网络中的算子 Z 是什么？
3.  关系函数是什么？
4.  关系网络的损失函数是什么？
5.  匹配网络中使用哪些不同类型的嵌入函数？
6.  如何在匹配网络中预测查询点的类别？

# 进一步阅读

*   [匹配网络](https://arxiv.org/pdf/1606.04080.pdf)
*   [关系网络](https://arxiv.org/pdf/1711.06025.pdf)