

# 评估






# 第 1 章



1.  无监督学习可以独立于有监督的方法应用，因为其目标是不同的。 如果问题需要监督的方法，则通常不能采用无监督的学习作为替代解决方案。 通常，无监督方法尝试从数据集中提取信息片段（例如，聚类）而没有任何外部提示（例如预测错误）。 相反，受监督的方法需要提示才能更正其参数。
2.  由于目标是找到趋势的原因，因此有必要执行诊断分析。
3.  没有; 从单个分布中提取 *n 个*独立样本的可能性作为单个概率的乘积（主要假设请参见问题 4）。
4.  主要假设是样本是**独立且** **均匀分布的**（**IID**）。
5.  性别可以编码为数字特征（例如，一键编码）； 因此，我们需要考虑两种可能性。 如果在属性之间不存在性别，而其他特征与性别不相关，则聚类结果是完全合理的。 如果存在性别（作为通用聚类方法）是基于样本之间的相似性的，则 50/50 的结果表示性别不是歧视性特征。 换句话说，给定两个随机选择的样本，它们的相似性不受性别影响（或受到轻微影响），因为其他特征占主导。 例如，在这种特殊情况下，平均分数或年龄有较大的差异，因此它们的影响更大。
6.  我们可以预期会有更紧凑的群体，其中每个主要特征的范围都较小。 例如，一个小组可以包含 13-15 岁的学生，并带有所有可能的分数，依此类推。 另外，我们可以观察基于单个特征的细分（例如年龄，平均分数等）。 最终结果取决于矢量的数值结构，距离函数，当然还取决于算法。

7.  如果每个客户均由包含其兴趣摘要的特征向量表示（例如，基于他/她购买或看过的产品），我们可以找到集群分配，检查哪些元素可以表征集群（对于 例如书籍，电影，衣服，特定品牌等），并使用这些信息来推荐潜在产品（即类似用户购买的产品）。 该概念基于在相同集群的成员之间共享信息的主要思想，这要归功于它们的相似性。





# 第 2 章



1.  曼哈顿距离与 Minkowski 距离相同，其中 *p = 1* ； 因此，我们希望观察到更长的距离。
2.  没有; 收敛速度主要受质心的初始位置影响。
3.  是; k-means 设计用于凸簇，而对于凹簇则性能较差。
4.  这意味着所有聚类（样本百分比可忽略不计）分别仅包含属于同一类别（即具有相同真实标签）的样本。
5.  它表示真实标签分配和分配之间的中等/强烈的负差异。 这个值是明显的负条件，不能接受，因为绝大多数样本已分配给错误的聚类。
6.  不可以，因为调整后的 Rand 分数是根据地面真实情况得出的（也就是说，预期的簇数是固定的）。
7.  如果所有基本查询都需要相同的时间，则会在 *60-（2×4）-2 = 50* 秒内执行它们。 因此，它们每个都需要 *50/100 = 0.5* 秒。 在*叶子大小= 50* 的情况下，我们可以期望将 *50-NN* 查询的执行时间减半，而对基本查询没有影响。 因此，可用于基本查询的总时间变为 *60-（2×2）-2 = 54* 秒。 因此，我们可以执行 *108* 基本查询。
8.  没有; 球树是一种不会遭受维度诅咒的数据结构，其计算复杂度始终为 *O（N log M）*。

9.  高斯 *N（[-1.0，0.0]，diag [0.1，0.2]）* 和 *N（[-0.8，0.0 ]，diag [0.3，0.3]）*重叠（即使所得聚类非常伸展），而第三个则足够远（考虑均值和方差），可以被单独的聚类捕获。 因此，最佳簇大小为 2，而 k 均值很难将大斑点正确地分为两个内聚分量（特别是对于大量样本）。
10.  VQ 是一种有损压缩方法。 仅当语义没有通过小或中转换而改变时，才可以使用它。 在这种情况下，如果不修改基础语义就不可能与另一个交换令牌。





# 第 3 章



1.  没有; 在凸集中，给定两个点，连接它们的线段始终位于该集内。
2.  考虑到数据集的径向结构，RBF 内核通常可以解决该问题。
3.  在*ε= 1.0* 的情况下，许多点无法达到密度。 当球的半径减小时，我们应该期望有更多的噪点。
4.  没有; k 型参量可以采用任何度量。
5.  没有; DBSCAN 对几何不敏感，并且可以管理任何种类的群集结构。
6.  我们已经表明，小批量 K 均值的性能稍差于 k 均值。 因此，答案是肯定的。 使用批处理算法可以节省内存。
7.  考虑到噪声的方差为*σ <sup class="calibre27">2</sup> = 0.005→σ≈0.07* ，它比聚类标准偏差小约 14 倍，因此，我们不能期望有这么多的新 在稳定的群集配置中分配（80％）。





# 第四章



1.  在凝聚方法中，该算法从每个样本（被视为一个集群）开始，然后继续合并子集群，直到定义了一个集群。 在分裂方法中，该算法从包含所有样本的单个簇开始，然后通过拆分将其进行到每个样本组成一个簇为止。

2.  最近的点是*（0，0）*和*（0，1）*，因此单键是 *L <sub class="calibre20">s</sub> （a，b）= 1* 。 最远的点是*（-1，-1）*和*（1、1）*，因此完整的链接是 *L <sub class="calibre20">c</sub> （a，b ）=2√2*。
3.  没有; 树状图是给定度量和链接的分层聚类过程的树表示。
4.  在聚集聚类中，树状图的初始部分包含所有样本作为自治聚类。
5. `y`轴报告差异。
6.  将较小的群集合并为较大的群集时，差异性会增加。
7.  是; 那就是共情矩阵的定义。
8.  连接性约束允许施加约束，因此将约束合并到聚合过程中，从而迫使其将某些元素保留在同一群集中。





# 第五章



1.  硬聚类基于固定分配； 因此，样本 *x <sub class="calibre20">i</sub>* 将始终属于单个群集。 相反，相对于每个聚类，软聚类返回一个度向量，该向量的元素表示隶属度（例如，（0.1、0.7、0.05、0.15））。
2.  没有; 模糊 c 均值是 k 均值的扩展，它不适用于非凸几何。 但是，软分配可以评估相邻群集的影响。
3.  主要假设是，数据集是从可以用多个高斯分布的加权和有效地近似的分布中得出的。
4.  这意味着第一个模型的参数数量是第二个模型的两倍。
5.  第二个是因为它可以用更少的参数实现相同的结果。
6.  因为我们要为组件的自动选择采用这种模型。 这意味着我们要从更大数量的权重开始，期望它们中的许多权重将被迫接近 0。由于 Dirichlet 分布具有非常稀疏的性质并且适用于单纯形，因此，这是最佳的选择。 先验。
7.  如果它们是从相同的来源收集的，并且已验证标记的来源，我们可以采用半监督方法（例如，生成高斯混合物），以便为其余样品找到最合适的标记。





# 第六章



1.  由于随机变量显然是独立的，因此 *P（高，雨）= P（高）P（雨）= 0.75•0.2 = 0.15。*
2.  直方图的主要缺点之一是，当 bin 的数量太大时，它们中的许多都开始为空，因为在所有值范围内都没有样本。 在这种情况下，`X`的基数可以小于 1,000，或者即使具有超过 1,000 个样本，相对频率也可以集中在小于 1,000 的多个 bin 中。
3.  样本总数为 75，并且各个条带的长度相等。 因此， *P（0 < x < 2）= 20/75≈0.27，P（2 < x < 4）= 30/75 = 0.4* 和 *P（4 < x < 6）= 25/75≈0.33* 。 由于我们没有任何样本，因此我们可以假设 *P（x > 6）= 0* ； 因此， *P（x > 2）=* *P（2 < x < 4）+ P（4 < x < 6）≈0.73* 。 考虑到 *0.73•75≈55* ，这是属于 *x > 2* 的 bin 的样本数，我们立即得到确认。
4.  在正态分布 *N（0，1）*中，最大密度为 *p（0）≈0.4* 。 在大约三个标准差之后， *p（x）≈0* ； 因此，通常无法将样本 *p（x）= 0.35* 的样本`x`视为异常。
5.  当 *min* （ *std（X），IQR（X）/1.34）≈2.24* 时，最佳带宽为 *h = 0.9•2.24•500 <sup class="calibre27">-0.2 [</sup> = 0.58* 。
6.  即使可以采用高斯核，在给出分布描述的情况下，我们也应首先选择指数核，这样可以使均值周围迅速下降。
7.  这将是最合乎逻辑的结论。 实际上，就新颖性而言，我们也应该期望新样本会改变分布，以便为新颖性建模。 如果在重新训练模型后概率密度仍然很低，则样本很可能是异常的。





# 第七章



1.  协方差矩阵已经是对角线； 因此，特征向量是标准`x`和`y`versors（1,0）和（0，1），特征值是 2 和 1。因此，`x`轴是主要成分，`y`轴是第二个成分。
2.  由于球 *B <sub class="calibre20">0.5</sub> （0，0）*是空的，因此在该点（ *0，0* ）周围没有样品。 考虑到水平方差*σ <sub class="calibre20">x</sub> <sup class="calibre27">2</sup> = 2* ，我们可以想象`X`被分解为两个斑点，因此可以想象 *x = 0* 行是水平鉴别符。 但是，这只是一个假设，需要使用实际数据进行验证。

3.  不，他们不是。 PCA 之后的协方差矩阵不相关，但不能保证统计独立性。
4.  是; Kurt（`X`）的分布是超高斯分布，因此达到峰值且尾巴很重。 这样可以保证找到独立的组件。
5.  由于`X`包含负数，因此无法使用 NNMF 算法。
6.  没有; 由于字典有 10 个元素，因此意味着文档由许多重复出现的术语组成，因此字典不够完整（ *10 < 30* ）。
7.  样本*（x，y）∈ <sup class="calibre27">2</sup>* 通过二次多项式变换为*（ax，by，cx <sup class="calibre27">2</sup> ，dy <sup class="calibre27">2</sup> ，exy，f）∈ <sup class="calibre27">6</sup>* 。





# 第八章



1.  不，他们没有。 编码器和解码器都必须在功能上对称，但是它们的内部结构也可以不同。
2.  没有; 输入信息的一部分在转换过程中丢失，而其余部分则在代码输出`Y`和自动编码器变量之间分配，该变量与基础模型一起对所有转换进行编码。
3.  当 *min（sum（z <sub class="calibre20">i</sub> ））= 0 和 min（sum（ z <sub class="calibre20">i</sub>* *））= 128* 时，等于 36 的总和既可以表示稀疏（如果标准偏差较大），也可以表示具有较小值的均匀分布（当标准偏差接近零时）。
4.  当 *sum（z <sub class="calibre20">i</sub> ）= 36 时，* a *std（z <sub class="calibre20">i</sub> ）= 0.03* 意味着大多数值都围绕 0.28 *（0.25÷0.31）*，该代码可以视为密集代码。
5.  没有; 一个 Sanger 网络（以及 Rubner-Tavan 网络）需要输入样本 *x <sub class="calibre20">i</sub> ∈X* 。
6.  从最大特征值到最小特征值（即从第一个主成分到最后一个主成分）以降序提取成分。 因此，无需进一步分析来确定其重要性。
7.  是; 从最后一层开始，可以对每个内部层的值进行采样，直到第一层为止。 通过选择每个概率向量的 *argmax（•）*获得最可能的输入值。





# 第九章



1.  没有; 生成器和鉴别器在功能上是不同的。
2.  不，不是这样，因为鉴别器的输出必须是一个概率（即 *p <sub class="calibre20">i</sub> ∈（0，1）*）。
3.  是; 这是正确的。 鉴别器可以学习非常快地输出不同的概率，，其损失函数的斜率可以变得接近于 0，从而减小了提供给发生器的校正反馈的幅度。
4.  是; 通常会比较慢。
5.  评论者比较慢，因为每次更新后都会对变量进行裁剪。
6.  由于支撑脱节，Jensen-Shannon 散度等于 *log（2）*。
7.  目标是开发高度选择性的单元，其响应仅由特定功能集引起。
8.  在培训过程的早期阶段，不可能知道最终的组织。 因此，强制某些单元的过早专业化不是一个好习惯。 调整阶段允许许多神经元成为候选神经元，与此同时，逐渐增加最有前途的神经元（将成为赢家）的选择性。


