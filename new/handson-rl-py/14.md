# 评估

# 第 1 章

1.  强化学习（**RL**）是机器学习的一个分支，其中学习是通过与环境交互来进行的。
2.  与其他 ML 范例不同，RL 通过训练和错误方法工作。
3.  代理是做出明智决策的软件程序，它们基本上是 RL 的学习者。
4.  策略函数指定在每个状态下要执行的操作，而值函数指定每个状态的值。
5.  在基于模型的代理中，使用以前的经验，而在无模型的学习中，则不会有任何以前的经验。
6.  确定性的，随机的，完全可观察的，部分可观察的，离散的连续的，事件的和非事件的。
7.  OpenAI Universe 为培训 RL 代理提供了丰富的环境。
8.  请参阅 RL 的*部分。*

# 第 2 章

1.  `conda create --name universe python=3.6 anaconda`
2.  使用 Docker，我们可以将应用程序及其依赖关系打包，称为容器，并且我们可以在服务器上运行应用程序，而无需将任何外部依赖关系与打包的 Docker 容器一起使用。
3.  `gym.make(env_name)`
4.  `from gym import envs`
    `print(envs.registry.all())`
5.  OpenAI Universe 是 OpenAI Gym 的扩展，还提供各种丰富的环境。
6.  占位符用于提供外部数据，而变量用于保持值。

7.  TensorFlow 中的所有内容都将表示为由节点和边组成的计算图，其中节点是数学运算（例如加法，乘法等），而边是张量。
8.  计算图只会被定义； 为了执行计算图，我们使用 TensorFlow 会话。

# 第 3 章

1.  马尔可夫性质指出，未来仅取决于现在而不是过去。
2.  MDP 是马尔可夫链的延伸。 它提供了用于建模决策情况的数学框架。 几乎所有的 RL 问题都可以建模为 MDP。
3.  请参阅*折扣系数*部分。
4.  折扣系数决定了我们对未来奖励和即时奖励的重视程度。
5.  我们使用 Bellman 函数求解 MDP。
6.  有关值和 Q 函数的信息，请参见*部分的推导 Bellman 方程。*
7.  值函数指定状态的优劣，而 Q 函数指定状态下的行为的优劣。
8.  请参阅*值迭代*和*策略迭代*部分。

# 第四章

1.  当环境模型未知时，在 RL 中使用 Monte Carlo 算法。
2.  请参阅*部分，使用蒙特卡洛*估算 pi 的值。
3.  在蒙特卡洛预测中，我们通过取均值回报而不是期望回报来近似值函数。
4.  在蒙特卡洛的每次访问中，我们平均将情节中每次访问州的收益均值化。 但是在首次访问 MC 方法中，我们仅在情节中首次访问状态时才对返回值进行平均。

5.  请参阅*蒙特卡洛控制*部分。
6.  请参阅*上策略的蒙特卡洛控制*和*上的策略外的蒙特卡洛控制*部分
7.  请参阅*部分，让我们和 Monte Carlo* 一起玩二十一点。

# 第五章

1.  蒙特卡罗方法仅适用于情节任务，而 TD 学习可应用于情节任务和非情节任务
2.  实际值与预测值之差称为 TD 误差
3.  请参阅 *TD 预测*和 *TD 控制*部分
4.  请参阅*部分，使用 Q 学习*解决滑行问题
5.  在 Q 学习中，我们使用 epsilon-greedy 策略采取行动，并且在更新 Q 值的同时，我们仅采取最大行动。 在 SARSA 中，我们使用 epsilon-greedy 策略采取措施，并且在更新 Q 值的同时，我们使用 epsilon-greedy 策略采取措施。

# 第六章

1.  MAB 实际上是一台老虎机，是一种在赌场玩的赌博游戏，您可以拉动手臂（杠杆）并根据随机生成的概率分布获得支出（奖励）。 一台老虎机称为单臂老虎机，当有多台老虎机时，称为多臂老虎机或 k 臂老虎机。
2.  当业务代表不确定是使用以前的经验来探索新动作还是利用最佳动作时，就会出现探索-利用困境。
3.  ε用于确定代理是否应使用 1-ε进行探索或利用我们选择最佳作用的作用，而使用ε则探索新作用。
4.  我们可以使用各种算法（例如 epsilon-greedy 策略，softmax 探索，UCB，Thompson 采样）解决探索-利用难题。
5.  UCB 算法可帮助我们根据置信区间选择最佳分支。
6.  在 Thomson 抽样中，我们使用先验分布进行估算，而在 UCB 中，我们使用置信区间进行估算。

# 第七章

1.  在神经元中，我们通过应用称为激活或传递函数的函数 *f（）*将非线性引入结果`z`。 请参阅*人工神经元*部分。
2.  激活函数用于引入非线性。
3.  我们计算成本函数相对于权重的梯度以最小化误差。
4.  RNN 不仅基于当前输入，而且还基于先前的隐藏状态来预测输出。
5.  在网络反向传播时，如果梯度值变得越来越小，则称为消失梯度问题，如果梯度值变得更大，则它正在爆炸梯度问题。
6.  门是 LSTM 中的特殊结构，用于决定保留，丢弃和更新哪些信息。
7.  池化层用于减少特征图的维数，并且仅保留必要的细节，因此可以减少计算量。

# 第八章

1.  **深度 Q 网络**（**DQN**）是用于逼近 Q 函数的神经网络。
2.  经验重播用于删除座席经验之间的相关性。
3.  当我们使用同一网络来预测目标值和预测值时，会有很多差异，因此我们使用单独的目标网络。
4.  由于最大运算符，DQN 高估了 Q 值。
5.  通过具有两个单独的 Q 函数，每个学习都独立地加倍 DQN，从而避免了高估 Q 值的情况。
6.  体验是优先体验回放中基于 TD 错误的优先级。
7.  DQN 通过将 Q 函数计算分解为值函数和优势函数来精确估算 Q 值。

# 第九章

1.  DRQN 利用**递归神经网络**（**RNN**），其中 DQN 利用香草神经网络。
2.  当可以部分观察 MDP 时，不使用 DQN。
3.  请参阅*带有 DRQN* 的厄运部分。
4.  与 DRQN 不同，DARQN 利用注意力机制。
5.  DARQN 用于理解和专注于游戏屏幕的特定区域，这一点更为重要。
6.  软硬注意。
7.  即使该举动无用，我们也将代理商的每次举动设置为活奖赏 0。

# 第 10 章

1.  A3C 是“异步优势参与者关键网络”，它使用多个代理进行并行学习。
2.  三个 A 是异步，优势，演员评论家。
3.  与 DQN 相比，A3C 需要更少的计算能力和训练时间。
4.  所有代理（员工）都在环境副本中工作，然后全球网络汇总他们的经验。
5.  熵用于确保足够的探索。
6.  请参阅*部分，A3C 的工作方式*。

# 第十一章

1.  策略梯度是 RL 中令人惊奇的算法之一，在该算法中，我们直接优化由某些参数设置参数的策略。
2.  策略梯度是有效的，因为我们无需计算 Q 函数即可找到最佳策略。
3.  Actor 网络的作用是通过调整参数来确定状态中的最佳动作，而 Critic 的作用是评估 Actor 产生的动作。

4.  请参阅*部分，信任区域策略优化*
5.  我们迭代地改进了该策略，并施加了一个约束，即旧策略和新策略之间的 **Kullback-Leibler**（**KL**）差异要小于某个常数。 该约束称为信任区域约束。
6.  PPO 通过将约束更改为惩罚项来修改 TRPO 的目标函数，因此我们不想执行共轭梯度。

# 第十二章

1.  DQN 直接计算 Q 值，而决斗 DQN 将 Q 值计算分解为值函数和优势函数。
2.  请参阅*重播存储器*部分。
3.  当我们使用同一网络来预测目标值和预测值时，会有很多差异，因此我们使用单独的目标网络。
4.  请参阅*重播存储器*部分。
5.  请参阅*决斗网络部分。*
6.  决斗 DQN 将 Q 值计算分解为值函数和优势函数，而双 DQN 使用两个 Q 函数来避免高估。
7.  请参阅“决斗*网络*”部分。

# 第十三章

1.  代理中的想象力指定了采取任何行动之前的可视化和计划。
2.  想象力核心由执行想象力的政策网络和环境模型组成。
3.  代理人反复从人类那里获得反馈，并根据人类的喜好改变目标。
4.  DQfd 使用一些演示数据进行培训，因为 DQN 并未预先使用任何演示数据。
5.  请参阅**后见之明体验回放**（**HER**）部分。

6.  提出了**分层强化学习**（**HRL**），以解决维数诅咒，其中我们将大问题解压缩为层次结构中的小子问题
7.  我们试图在 RL 中找到给定奖励函数的最优策略，而在逆向强化学习中，给出最优策略并找到奖励函数