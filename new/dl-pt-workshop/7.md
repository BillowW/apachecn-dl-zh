# 附录

# 1.深度学习和 PyTorch 简介

## 活动 1.01：创建单层神经网络

### 解决方案

1.  导入所需的库，包括pandas，用于导入CSV文件。

    ```py
    import pandas as pd
    import torch
    import torch.nn as nn
    import matplotlib.pyplot as plt
    ```

2.  读取包含数据集的CSV文件。

    ```py
    data = pd.read_csv("SomervilleHappinessSurvey2015.csv")
    ```

3.  将输入特征与目标分开。注意，目标位于CSV文件的第一列。将值转换为张量，确保值转换为浮点数。

    ```py
    x = torch.tensor(data.iloc[:,1:].values).float()
    y = torch.tensor(data.iloc[:,:1].values).float()
    ```

4.  定义模型的架构，并将其存储在一个名为`model`的变量中。记住要创建一个单层模型。

    ```py
    model = nn.Sequential(nn.Linear(6, 1),
                          nn.Sigmoid())
    ```

5.  定义要使用的损失函数。使用MSE损失函数。

    ```py
    loss_function = torch.nn.MSELoss()
    ```

6.  定义你模型的优化器。使用亚当优化器和学习率`0.01`。

    ```py
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    ```

7.  运行优化100次迭代。每迭代10次，打印并保存损失值。

    ```py
    losses = []
    for i in range(100):
        y_pred = model(x)
        loss = loss_function(y_pred, y)
        losses.append(loss.item())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if i%10 == 0:
            print(loss.item())
    ```

    最终损失应约为`0.24`。

8.  做一个线图来显示每个迭代步骤的损失值。

    ```py
    plt.plot(range(0,100), losses)
    plt.show()
    ```

    结果图应如下所示：

![Figure 1.4: Loss function throughout the training process ](img/B15778_01_04.jpg)

图 1.4：整个训练过程中的损失功能

这意味着训练过程能够使损失函数最小化，这意味着结果模型将可能能够绘制出市民对城市服务的满意度与他们对行政管理是否满意之间的关系。

注意

要访问此特定部分的源代码，请参考[这里](https://packt.live/2ZufWiI)。

您也可以通过[这里](https://packt.live/2BZhyZF)在线运行此示例。 您必须执行整个笔记本才能获得所需的结果。

# 2.神经网络的构建基块

## 活动 2.01：执行数据准备

### 解决方案

1.  导入所需的库。

    ```py
    import pandas as pd
    ```

2.  使用pandas，加载`.csv`文件。

    ```py
    data = pd.read_csv("YearPredictionMSD.csv", nrows=50000)
    data.head()
    ```

    注意

    为避免内存限制，在读取文本文件时，请使用`nrows`自变量，以读取整个数据集的较小部分。 在前面的示例中，我们正在读取前 50,000 行。

    输出如下：

    ![Figure 2.33: YearPredictionMSD.csv ](img/B15778_02_33.jpg)

    图 2.33：YearPredictionMSD.csv

3.  核实数据集中是否存在任何定性数据。

    ```py
    cols = data.columns
    num_cols = data._get_numeric_data().columns
    list(set(cols) - set(num_cols))
    ```

    输出应为空列表，这意味着没有定性特征。

4.  检查是否有缺失值。

    如果在先前用于此目的的代码行中添加一个附加的`sum()`函数，则将获得整个数据集中的缺失值之和，而无需按列进行区分：

    ```py
    data.isnull().sum().sum()
    ```

    输出应为`0`，这意味着所有要素均不包含缺失值。

5.  检查是否有异常值。

    ```py
    outliers = {}
    for i in range(data.shape[1]):
        min_t = data[data.columns[i]].mean() \
                - (3 * data[data.columns[i]].std())
        max_t = data[data.columns[i]].mean() \
                + (3 * data[data.columns[i]].std())
        count = 0
        for j in data[data.columns[i]]:
            if j < min_t or j > max_t:
                count += 1
        percentage = count/data.shape[0]
        outliers[data.columns[i]] = "%.3f" % percentage
    print(outliers)
    ```

    输出字典应显示所有要素均不包含代表超过 5% 数据的离群值。

6.  将特征从目标数据中分离出来。

    ```py
    X = data.iloc[:, 1:]
    Y = data.iloc[:, 0]
    ```

7.  使用标准化方法对特征数据进行重新标定。

    ```py
    X = (X - X.mean())/X.std()
    X.head()
    ```

    输出如下：

    ![Figure 2.34: Rescaled features data ](img/B15778_02_34.jpg)

    图 2.34：重新缩放的要素数据

8.  将数据分成三组：训练、验证和测试。使用你喜欢的方法。

    ```py
    from sklearn.model_selection import train_test_split
    X_shuffle = X.sample(frac=1, random_state=0)
    Y_shuffle = Y.sample(frac=1, random_state=0)
    x_new, x_test, \
    y_new, y_test = train_test_split(X_shuffle, \
                                     Y_shuffle, \
                                     test_size=0.2, \
                                     random_state=0)
    dev_per = x_test.shape[0]/x_new.shape[0]
    x_train, x_dev, \
    y_train, y_dev = train_test_split(x_new, \
                                      y_new, \
                                      test_size=dev_per, \
                                      random_state=0)
    ```

9.  打印所得形状如下。

    ```py
    print(x_train.shape, y_train.shape)
    print(x_dev.shape, y_dev.shape)
    print(x_test.shape, y_test.shape)
    ```

    输出应如下所示：

    ```py
    (30000, 90) (30000, )
    (10000, 90) (10000, )
    (10000, 90) (10000, )
    ```

    注意

    要访问此特定部分的源代码，请参考[这里](https://packt.live/31ukVTj)。

    您也可以通过[这里](https://packt.live/3dLWMdd)在线运行此示例。 您必须执行整个笔记本才能获得所需的结果。

## 活动 2.02：为回归问题开发深度学习解决方案

### 解决方案

1.  导入所需的库。

    进口火炬

    将 torch.nn 导入为 nn

2.  将我们在上一个活动中创建的所有三组数据的特征从目标中分割出来。将DataFrames转换为张量。

    x_train = torch.tensor（x_train.values）.float（）

    y_train =火炬张量（y_train.values）.float（）

    x_dev = torch.tensor（x_dev.values）.float（）

    y_dev = torch.tensor（y_dev.values）.float（）

    x_test = torch.tensor（x_test.values）.float（）

    y_test = torch.tensor（y_test.values）.float（）

3.  定义网络的架构。可以自由尝试不同的层数和每层单元数的组合。

    模型= nn.Sequential（nn.Linear（x_train.shape [1]，10），\

    nn.ReLU（），\

    nn.Linear（10，7），\

    nn.ReLU（），\

    nn.Linear（7，5），\

    nn.ReLU（），\

    nn.Linear（5，1））

4.  定义损失函数和优化器算法。

    loss_function = torch.nn.MSELoss（）

    优化程序= torch.optim.Adam（model.parameters（），lr = 0.01）

5.  使用`for`循环来训练网络，迭代步数为3000步。

    对于我的范围（3000）：

    y_pred = model（x_train）.squeeze（）

    损失= loss_function（y_pred，y_train）

    Optimizer.zero_grad（）

    loss.backward（）

    Optimizer.step（）

    如果 i% 250 == 0：

    打印（i，loss.item（））

6.  通过对测试集的第一个实例进行预测，并与地面真相进行比较来测试你的模型。

    之前=模型（x_test [0]）

    print（“地面真相：”，y_test [0] .item（），\

    “预测：”，pred.item（））

    您的输出应类似于以下内容：

    基本事实：1995.0 预测：1998.0279541015625

    注意

    要访问此特定部分的源代码，请参考[这里](https://packt.live/2CUDSnP)。

    您也可以通过[这里](https://packt.live/3eQ1yI2)在线运行此示例。 您必须执行整个笔记本才能获得所需的结果。

# 3.使用 DNN 的分类问题

## 活动 3.01：构建人工神经网络

解：

1.  导入以下库：

    将熊猫作为 pd 导入

    将 numpy 导入为 np

    从 sklearn.model_selection 导入 train_test_split

    从 sklearn.utils 导入 shuffle

    从 sklearn.metrics 导入 precision_score

    进口火炬

    从火炬进口 nn，乐观

    导入功能为 F 的 torch.nn。

    导入 matplotlib.pyplot 作为 plt

    torch.manual_seed（0）

2.  读取之前准备好的数据集，该数据集应该命名为`dccc_prepared.csv`。

    数据= pd.read_csv（“ dccc_prepared.csv”）

    data.head（）

    输出应如下所示：

    ![Figure 3.14: dccc_prepared.csv ](img/B15778_03_14.jpg)

    图 3.14：dccc_prepared.csv

3.  将特征与目标分开。

    X = data.loc [：，：-1]

    y =数据[“下个月的默认付款”]

4.  使用scikit-learn的`train_test_split`函数，将数据集分割成训练集、验证集和测试集。使用60:20:20的分割比例。将`random_state`设置为0。

    X_new，X_test，\

    y_new，y_test = train_test_split（X，y，test_size = 0.2，\

    random_state = 0）

    dev_per = X_test.shape [0] /X_new.shape [0]

    X_train，X_dev，\

    y_train，y_dev = train_test_split（X_new，y_new，\

    test_size = dev_per，\

    random_state = 0）

    您可以使用以下代码打印每个集合的最终形状：

    print（“训练集：”，X_train.shape，y_train.shape）

    print（“验证集：”，X_dev.shape，y_dev.shape）

    print（“测试集：”，X_test.shape，y_test.shape）

    每个集合的最终形状如下所示：

    训练集：（28036，22）（28036，）

    验证集：（9346，22）（9346，）

    测试集：（9346，22）（9346，）

5.  将验证集和测试集转换为张量，记住特征矩阵应该是`float`类型，而目标矩阵不应该。训练集暂不转换，因为它们将进行进一步的转换。

    X_dev_torch = torch.tensor（X_dev.values）.float（）

    y_dev_torch = torch.tensor（y_dev.values）

    X_test_torch = torch.tensor（X_test.values）.float（）

    y_test_torch =火炬张量（y_test.values）

6.  构建一个自定义模块类，用于定义网络的层。包括一个前向函数，指定将应用于每层输出的激活函数。对所有层都使用 **ReLU**，除了输出，你应该使用`log_softmax`。

    类 Classifier（nn.Module）：

    def __init __（self，input_size）：

    super（）.__ init __（）

    self.hidden_​​1 = nn.Linear（input_size，10）

    self.hidden_​​2 = nn.Linear（10，10）

    self.hidden_​​3 = nn.Linear（10，10）

    self.output = nn.Linear（10，2）

    def forward（self，x）：

    z = F.relu（self.hidden_​​1（x））

    z = F.relu（self.hidden_​​2（z））

    z = F.relu（self.hidden_​​3（z））

    out = F.log_softmax（self.output（z），dim = 1）

    返回

7.  实例化模型并定义训练模型所需的所有变量。设置纪元数为`50`，批次大小为`128`。使用`0.001`的学习率。

    模型=分类器（X_train.shape [1]）

    准则= nn.NLLLoss（）

    优化程序= optim.Adam（model.parameters（），lr = 0.001）

    时代= 50

    batch_size = 128

8.  使用训练集的数据来训练网络。使用验证集来衡量性能。要做到这一点，请保存每个时代的训练集和验证集的损失和准确性。

    train_losses，dev_losses，

    train_acc，dev_acc = []，[]，[]，[]

    对于范围内的 e（历元）：

    X_，y_ = shuffle（X_train，y_train）

    running_loss = 0

    running_acc = 0

    迭代次数= 0

    对于范围（0，len（X_），batch_size）中的 i：

    迭代次数== 1

    b =我+ batch_size

    X_batch = torch.tensor（X_.iloc [i：b，：]。values）.float（）

    y_batch = torch.tensor（y_.iloc [i：b] .values）

    pred =模型（X_batch）

    损失=标准（pred，y_batch）

    Optimizer.zero_grad（）

    loss.backward（）

    Optimizer.step（）

    running_loss + = loss.item（）

    ps = torch.exp（pred）

    top_p，top_class = ps.topk（1，暗= 1）

    running_acc + =精度得分（y_batch，top_class）

    dev_loss = 0

    acc = 0

    使用 torch.no_grad（）：

    pred_dev =模型（X_dev_torch）

    dev_loss =条件（pred_dev，y_dev_torch）

    ps_dev = torch.exp（pred_dev）

    top_p，top_class_dev = ps_dev.topk（1，暗= 1）

    acc = precision_score（y_dev_torch，top_class_dev）

    train_losses.append（running_loss / iterations）

    dev_losses.append（dev_loss）

    train_acc.append（running_acc / iterations）

    dev_acc.append（acc）

    print（“ Epoch：{} / {} ..” .format（e + 1，epochs），\

    “训练损失：{:. 3f} ..” \

    .format（running_loss / iterations），\

    “验证损失：{:. 3f} ..” .format（dev_loss），\

    “训练准确率：{:. 3f} ..” \

    .format（running_acc / iterations），\

    “验证准确率：{:. 3f}”。format（acc））

9.  绘出两组的损失。

    图 = plt.figure（figsize =（15，5））

    plt.plot（train_losses，label ='训练损失'）

    plt.plot（dev_losses，label ='Validation loss'）

    plt.legend（frameon = False，fontsize = 15）

    plt.show()

    考虑到改组训练数据可能会得出略有不同的结果，结果图应与此处显示的图相似，尽管有所不同。

    ![Figure 3.15: A plot displaying the training and validation losses ](img/B15778_03_15.jpg)

    图 3.15：显示训练和验证损失的图

10.  绘制两组的精度。

    无花果= plt.figure（figsize =（15，5））

    plt.plot（train_acc，label =“训练精度”）

    plt.plot（dev_acc，label =“验证准确率”）

    plt.legend（frameon = False，fontsize = 15）

    plt.show()

    这是从此代码段派生的图：

![Figure 3.16: A plot displaying the accuracy of the sets ](img/B15778_03_16.jpg)

图 3.16：显示集合精度的图

注意

要访问此特定部分的源代码，请参考[这里](https://packt.live/2Vz6BoK)。

您也可以通过[这里](https://packt.live/2NNBuRS)在线运行此示例。 您必须执行整个笔记本才能获得所需的结果。

## 练习 3.02：提高模型的性能

解：

1.  导入你在上一个活动中使用的相同的库。

    将熊猫作为 pd 导入

    将 numpy 导入为 np

    从 sklearn.model_selection 导入 train_test_split

    从 sklearn.utils 导入 shuffle

    从 sklearn.metrics 导入 precision_score

    进口火炬

    从火炬进口 nn，乐观

    导入功能为 F 的 torch.nn。

    导入 matplotlib.pyplot 作为 plt

    torch.manual_seed（0）

2.  加载数据并从目标中拆分特征。接下来，使用 60:20:20 的分割比例将数据分割成三个子集（训练、验证和测试）。最后，将验证和测试集转换为 PyTorch 张量，就像您在上一个活动中所做的那样。

    数据= pd.read_csv（“ dccc_prepared.csv”）

    X = data.loc [：，：-1]

    y =数据[“下个月的默认付款”]

    X_new，X_test，\

    y_new，y_test = train_test_split（X，y，test_size = 0.2，\

    random_state = 0）

    dev_per = X_test.shape [0] /X_new.shape [0]

    X_train，X_dev，\

    y_train，y_dev = train_test_split（X_new，y_new，\

    test_size = dev_per，\

    random_state = 0）

    X_dev_torch = torch.tensor（X_dev.values）.float（）

    y_dev_torch = torch.tensor（y_dev.values）

    X_test_torch = torch.tensor（X_test.values）.float（）

    y_test_torch =火炬张量（y_test.values）

3.  考虑到该模型存在较高的偏差，重点应放在增加纪元的数量上，或通过在每层中增加额外的层或单位来增加网络的规模。目标应该是将验证集的准确度近似到80%。

    之后，将显示性能最佳的模型，该模型是在几次微调尝试之后实现的。 首先，定义模型架构和正向传递，如以下代码片段所示：

    类 Classifier（nn.Module）：

    def __init __（self，input_size）：

    super（）.__ init __（）

    self.hidden_​​1 = nn.Linear（input_size，100）

    self.hidden_​​2 = nn.Linear（100，100）

    self.hidden_​​3 = nn.Linear（100，50）

    self.hidden_​​4 = nn.Linear（50,50）

    self.output = nn.Linear（50，2）

    self.dropout = nn.Dropout（p = 0.1）

    def forward（self，x）：

    z = self.dropout（F.relu（self.hidden_​​1（x）））

    z = self.dropout（F.relu（self.hidden_​​2（z）））

    z = self.dropout（F.relu（self.hidden_​​3（z）））

    z = self.dropout（F.relu（self.hidden_​​4（z）））

    out = F.log_softmax（self.output（z），dim = 1）

    返回

    接下来，定义训练过程的不同参数。 这包括损失函数，优化算法，批量大小和时期数，如以下代码所示：

    模型=分类器（X_train.shape [1]）

    准则= nn.NLLLoss（）

    优化程序= optim.Adam（model.parameters（），lr = 0.001）

    时代= 4000

    batch_size = 128

    最后，按照以下代码片段处理训练过程：

    train_losses，dev_losses，train_acc，dev_acc = []，[]，[]，[]

    x_axis = []

    对于范围（1，历元+1）中的 e：

    X_，y_ = shuffle（X_train，y_train）

    running_loss = 0

    running_acc = 0

    迭代次数= 0

    对于范围（0，len（X_），batch_size）中的 i：

    迭代次数== 1

    b =我+ batch_size

    X_batch = torch.tensor（X_.iloc [i：b，：]。values）.float（）

    y_batch = torch.tensor（y_.iloc [i：b] .values）

    log_ps =模型（X_batch）

    损失=标准（log_ps，y_batch）

    Optimizer.zero_grad（）

    loss.backward（）

    Optimizer.step（）

    running_loss + = loss.item（）

    ps = torch.exp（log_ps）

    top_p，top_class = ps.topk（1，暗= 1）

    running_acc + =精度得分（y_batch，top_class）

    dev_loss = 0

    acc = 0

    使用 torch.no_grad（）：

    model.eval（）

    log_dev =模型（X_dev_torch）

    dev_loss =条件（log_dev，y_dev_torch）

    ps_dev = torch.exp（log_dev）

    top_p，top_class_dev = ps_dev.topk（1，暗= 1）

    acc = precision_score（y_dev_torch，top_class_dev）

    model.train（）

    如果 e% 50 == 0 或 e == 1：

    x_axis.append（e）

    train_losses.append（running_loss / iterations）

    dev_losses.append（dev_loss）

    train_acc.append（running_acc / iterations）

    dev_acc.append（acc）

    print（“ Epoch：{} / {} ..”“ .format（e，epochs），\

    “训练损失：{:. 3f} ..” \

    .format（running_loss / iterations），\

    “验证损失：{:. 3f} ..” .format（dev_loss），\

    “训练准确率：{:. 3f} ..” \

    .format（running_acc / iterations），\

    “验证准确率：{:. 3f}”。format（acc））

    注意

    可以在以前共享的 GitHub 存储库中找到此活动随附的 Jupyter Notebook。 在那里，您会发现对模型进行微调的各种尝试及其结果。 性能最佳的型号可以在笔记本电脑的末尾找到。

4.  绘制两组数据的损失和准确性。

    注意

    请记住，此处显示的结果与您的结果不完全匹配。 这主要是由于训练网络时使用了改组功能。

    使用以下代码绘制损失：

    无花果= plt.figure（figsize =（15，5））

    plt.plot（x_axis，train_losses，label ='训练损失'）

    plt.plot（x_axis，dev_losses，label ='验证损失'）

    plt.legend（frameon = False，fontsize = 15）

    plt.show()

    运行前面的代码将显示以下图：

    ![Figure 3.17: A plot displaying the loss of the sets ](img/B15778_03_17.jpg)

    图 3.17：显示集合损失的图

    使用以下代码来绘制精度：

    无花果= plt.figure（figsize =（15，5））

    plt.plot（x_axis，train_acc，label =“训练精度”）

    plt.plot（x_axis，dev_acc，label =“验证准确率”）

    plt.legend（frameon = False，fontsize = 15）

    plt.show()

    运行前面的代码将显示以下图：

    ![Figure 3.18: A plot displaying the accuracy of the sets ](img/B15778_03_18.jpg)

    图 3.18：显示集合精度的图

5.  使用性能最好的模型，对测试集（在微调过程中不应该使用）进行预测。通过计算模型在该集上的准确度，将预测结果与基本事实进行比较。

    model.eval（）

    test_pred =模型（X_test_torch）

    test_pred = torch.exp（test_pred）

    top_p，top_class_test = test_pred.topk（1，暗= 1）

    acc_test = precision_score（y_test_torch，top_class_test）

    打印（acc_test）

    通过模型架构和此处定义的参数获得的精度应为 80% 左右。

    注意

    要访问此特定部分的源代码，请参考[这里](https://packt.live/2Bs42hh)。

    本部分当前没有在线交互示例，需要在本地运行。

## 活动 3.03：使用模型

### 解决方案

1.  打开用于上一个活动的 Jupyter 笔记本。
2.  复制包含最佳性能模型架构的类，并将其保存在 Python 文件中。确保导入了 PyTorch 所需的库和模块，并将其命名为`final_model.py`。将其命名为`final_model.py`。

    该文件应如下所示：

    ![Figure 3.19: A screenshot of final_model.py ](img/B15778_03_19.jpg)

    图 3.19：final_model.py 的屏幕截图

3.  在Jupyter 笔记本中，保存表现最好的模型。请务必保存与输入单位相关的信息，以及模型的参数。将其命名为`checkpoint.pth`。

    检查点= {“输入”：X_train.shape [1]，\

    “ state_dict”：model.state_dict（）}

    torch.save（检查点，“ checkpoint.pth”）

4.  打开一个新的 Jupyter Notebook。
5.  导入PyTorch，以及我们在“步骤 2”中创建的Python文件。

    进口火炬

    导入 final_model

6.  创建一个加载模型的函数。

    def load_model_checkpoint（path）：

    检查点= torch.load（路径）

    模型= final_model.Classifier（checkpoint [“ input”]）

    model.load_state_dict（checkpoint [“ state_dict”]）

    退货模式

    模型= load_model_checkpoint（“ checkpoint.pth”）

7.  通过将以下张量输入到你的模型中进行预测。

    例子= torch.tensor（[[0.0606，0.5000，0.3333，0.4828，\

    0.4000, 0.4000, 0.4000, 0.4000, \

    0.4000, 0.4000, 0.1651, 0.0869, \

    0.0980, 0.1825, 0.1054, 0.2807, \

    0.0016, 0.0000, 0.0033, 0.0027, \

    0.0031，0.0021]]）。float（）

    之前=模型（示例）

    pred = torch.exp（pred）

    top_p，top_class_test = pred.topk（1，暗= 1）

    通过打印`top_class_test`，我们可以获得模型的预测，在这种情况下，该预测等于`1`（是）。

8.  使用JIT模块转换模型。

    traced_script = torch.jit.trace（模型，例如\

    check_trace = False）

9.  通过输入“步骤 7”的相同张量到模型的跟踪脚本中进行预测。

    预测= traced_script（示例）

    预测= torch.exp（预测）

    top_p_2，top_class_test_2 = projection.topk（1，dim = 1）

    通过打印`top_class_test_2`，我们从模型的跟踪脚本表示中获得了预测，该预测再次等于`1`（是）。

10.  打开一个新的Jupyter 笔记本，并导入所需的库来使用Flask创建一个API，以及加载保存的模型的库。

    进口烧瓶

    从烧瓶进口要求

    进口火炬

    导入 final_model

11.  初始化Flask应用。

    app = flask.Flask（__ name__）

    app.config [“ DEBUG”] = True

12.  定义一个函数，加载保存的模型，然后实例化模型。

    def load_model_checkpoint（path）：

    检查点= torch.load（路径）

    模型= final_model.Classifier（checkpoint [“ input”]）

    model.load_state_dict（checkpoint [“ state_dict”]）

    退货模式

    模型= load_model_checkpoint（“ checkpoint.pth”）

13.  定义API的路由为`/prediction`，并将方法设置为`POST`。然后，定义接收`POST`数据的函数，并将其反馈给模型进行预测。

    @ app.route（'/ prediction'，methods = ['POST']）

    def definition（）：

    正文= request.get_json（）

    示例= torch.tensor（body ['data']）。float（）

    之前=模型（示例）

    pred = torch.exp（pred）

    _，top_class_test = pred.topk（1，暗= 1）

    top_class_test = top_class_test.numpy（）

    return {“ status”：“ ok”，“ result”：int（top_class_test [0] [0]）}

14.  运行Flask应用。

    app.run（debug = True，use_reloader = False）

    使用为 API 开发而创建的平台 Postman，可以测试 API。 要向 Postman 提交成功的请求，标头的`*Content-Type`应当等于`application/json`。 结果输出应如下所示：

![Figure 3.20: A screenshot of the app after running it ](img/B15778_03_13.jpg)

图 3.20：应用程序运行后的屏幕截图

注意

要访问此特定部分的源代码，请参考[这里](https://packt.live/2NHkddn)。

本部分当前没有在线交互示例，需要在本地运行。

# 4.卷积神经网络

## 活动 4.01：针对图像分类问题构建 CNN

### 解决方案

1.  导入所需的库。

    将 numpy 导入为 np

    进口火炬

    从火炬进口 nn，乐观

    导入功能为 F 的 torch.nn。

    从 torchvision 导入数据集中

    导入 torchvision.transforms 作为转换

    从 torch.utils.data.sampler 导入 SubsetRandomSampler

    从 sklearn.metrics 导入 precision_score

    导入 matplotlib.pyplot 作为 plt

2.  设置要对数据进行的变换，将数据转换为张量并对像素值进行归一化。

    变换= \

    transforms.Compose（[[transforms.ToTensor（），\

    transforms.Normalize（（0.5，0.5，0.5），\

    (0.5, 0.5, 0.5))])

3.  设置批量大小为100张图像，并从 **CIFAR10** 数据集下载训练和测试数据。

    batch_size = 100

    train_data =数据集.CIFAR10（'data'，train = True，\

    download = True，\

    转换=转换）

    test_data =数据集.CIFAR10（'data'，train = False，\

    download = True，\

    转换=转换）

    前面的代码将下载可通过 PyTorch 的`Torchvision`软件包获得的训练和测试数据集。 根据上一步中定义的转换对数据集进行转换。

4.  使用 20% 的验证大小，定义训练和验证采样器，用于将数据集划分为这两组。

    dev_size = 0.2

    idx =列表（范围（len（train_data）））

    np.random.shuffle（idx）

    split_size = int（np.floor（dev_size * len（train_data）））

    train_idx，dev_idx = idx [split_size：]，idx [：split_size]

    train_sampler = SubsetRandomSampler（train_idx）

    dev_sampler = SubsetRandomSampler（dev_idx）

    为了将训练集分为两组（训练和验证），为每个组定义了一个索引列表，然后可以使用`SubsetRandomSampler`函数对其进行随机采样。

5.  使用`DataLoader()`函数来定义要使用的每一组数据的批次。

    train_loader = \

    torch.utils.data.DataLoader（train_data，\

    batch_size =批量大小，\

    sampler = train_sampler）

    dev_loader = \

    torch.utils.data.DataLoader（train_data，\

    batch_size =批量大小，\

    sampler = dev_sampler）

    test_loader = \

    torch.utils.data.DataLoader（test_data，\

    batch_size =批量大小）

    PyTorch 的`DataLoader`函数用于创建批量，这些批量将在开发过程的训练，验证和测试阶段馈送到模型中。

6.  定义你的网络架构。使用以下信息进行定义。

    Conv1：卷积层，将彩色图像作为输入，并将其通过大小为 3 的 10 个滤镜。应将 padding 和 stride 都设置为 1。

    Conv2：一个卷积层，它将输入数据通过大小为 3 的 20 个过滤器传递。填充和跨距都应设置为 1。

    Conv3：一个卷积层，它将输入数据通过大小为 3 的 40 个过滤器传递。填充和跨距都应设置为 1。

    在每个卷积层之后使用 ReLU 激活函数。

    在每个卷积层之后使用池化层，滤镜大小和步幅为 2。

    展平图像后，使用掉落项设置为 20%。

    线性 1：一个全连接层，接收上一层的展平矩阵作为输入，并生成 100 个单位的输出。 为此层使用 ReLU 激活函数。 此处的辍学期限设置为 20%。

    Linear2：一个全连接层，可生成 10 个输出，每个类标签一个。 将`log_softmax`激活函数用于输出层：

    CNN（nn.Module）类：

    def __init __（）：

    超级（CNN，自我）.__ init __（）

    self.conv1 = nn.Conv2d（3，10，3，1，1）

    self.conv2 = nn.Conv2d（10，20，3，1，1）

    self.conv3 = nn.Conv2d（20，40，3，1，1）

    self.pool = nn.MaxPool2d（2，2）

    self.linear1 = nn.Linear（40 * 4 * 4，100）

    self.linear2 = nn.Linear（100，10）

    self.dropout = nn.Dropout（0.2）

    def forward（self，x）：

    x = self.pool（F.relu（self.conv1（x）））

    x = self.pool（F.relu（self.conv2（x）））

    x = self.pool（F.relu（self.conv3（x）））

    x = x.view（-1，40 * 4 * 4）

    x = self.dropout（x）

    x = F.relu（self.linear1（x））

    x = self.dropout（x）

    x = F.log_softmax（self.linear2（x），dim = 1）

    返回 x

    前面的代码段包含一个定义了网络架构的类（`__init__`方法），以及在信息正向传递过程中所遵循的步骤（`forward`方法）。

7.  定义训练模型所需的所有参数。设置周期数为`50`。

    型号= CNN（）

    loss_function = nn.NLLLoss（）

    优化程序= optim.Adam（model.parameters（），lr = 0.001）

    时代= 50

    我们为此练习选择的优化器是 Adam。 同样，负对数似然率用作损失函数，如本书前一章所述。

    如果您的计算机具有可用的 GPU，则应按以下步骤完成模型的实例化：

    型号= CNN（）。to（“ cuda”）

8.  训练你的网络，并确保保存训练集和验证集的损失和准确性的值。

    train_losses，dev_losses，train_acc，dev_acc = []，[]，[]，[]

    x_axis = []

    ＃循环遍历

    对于范围（1，历元+1）中的 e：

    损失= 0

    acc = 0

    迭代次数= 0

    model.train（）

    """

    For 遍历批量（使用创建

    火车装载者）

    """

    对于数据，在 train_loader 中定位：

    迭代次数== 1

    ＃训练数据的前后传递

    pred =模型（数据）

    损失= loss_function（pred，目标）

    Optimizer.zero_grad（）

    loss.backward（）

    Optimizer.step（）

    损失+ = loss.item（）

    p = torch.exp（pred）

    top_p，top_class = p.topk（1，暗= 1）

    acc + = precision_score（target，top_class）

    dev_losss = 0

    dev_accs = 0

    iter_2 = 0

    ＃验证给定时期的模型

    如果 e% 5 == 0 或 e == 1：

    x_axis.append（e）

    使用 torch.no_grad（）：

    model.eval（）

    """

    用于循环遍历

    验证集

    """

    对于 dev_loader 中的 data_dev，target_dev：

    iter_2 + = 1

    dev_pred =模型（data_dev）

    dev_loss = loss_function（dev_pred，target_dev）

    dev_losss + = dev_loss.item（）

    dev_p = torch.exp（dev_pred）

    top_p，dev_top_class = dev_p.topk（1，暗= 1）

    dev_accs + = precision_score（target_dev，\

    dev_top_class）

    ＃损失和准确率将附加打印

    train_losses.append（损失/迭代）

    dev_losses.append（dev_losss / iter_2）

    train_acc.append（acc /迭代）

    dev_acc.append（dev_accs / iter_2）

    print（“ Epoch：{} / {} ..”“ .format（e，epochs），\

    “训练损失：{:. 3f} ..” \

    .format（损失/迭代），\

    “验证损失：{:. 3f} ..” \

    .format（dev_losss / iter_2），\

    “训练准确率：{:. 3f} ..” \

    .format（acc / iterations），\

    “验证准确率：{:. 3f}” \

    .format（dev_accs / iter_2））

    如果您的计算机具有可用的 GPU，则对前面的代码进行一些修改，如下所示：

    train_losses，dev_losses，train_acc，dev_acc = []，[]，[]，[]

    x_axis = []

    ＃循环遍历

    对于范围（1，历元+1）中的 e：

    损失= 0

    acc = 0

    迭代次数= 0

    model.train（）

    """

    循环遍历批量

    （使用火车装载程序创建）

    """

    对于数据，在 train_loader 中定位：

    迭代次数== 1

    ＃训练数据的前后传递

    pred =模型（data.to（“ cuda”））

    损失= loss_function（pred，target.to（“ cuda”））

    Optimizer.zero_grad（）

    loss.backward（）

    Optimizer.step（）

    损失+ = loss.item（）

    p = torch.exp（pred）

    top_p，top_class = p.topk（1，暗= 1）

    acc + = precision_score（target.to（“ cpu”），\

    top_class.to（“ cpu”））

    dev_losss = 0

    dev_accs = 0

    iter_2 = 0

    ＃验证给定时期的模型

    如果 e% 5 == 0 或 e == 1：

    x_axis.append（e）

    使用 torch.no_grad（）：

    model.eval（）

    """

    用于循环遍历

    验证集

    """

    对于 dev_loader 中的 data_dev，target_dev：

    iter_2 + = 1

    dev_pred =模型（data_dev.to（“奇迹”））

    dev_loss = loss_function（dev_pred，\

    target_dev.to（“ cuda”））

    dev_losss + = dev_loss.item（）

    dev_p = torch.exp（dev_pred）

    top_p，dev_top_class = dev_p.topk（1，暗= 1）

    dev_accs + = \

    precision_score（target_dev.to（“ cpu”），\

    dev_top_class.to（“ cpu”））

    ＃损失和准确率将附加打印

    train_losses.append（损失/迭代）

    dev_losses.append（dev_losss / iter_2）

    train_acc.append（acc /迭代）

    dev_acc.append（dev_accs / iter_2）

    print（“ Epoch：{} / {} ..”“ .format（e，epochs），\

    “训练损失：{:. 3f} ..” \

    .format（损失/迭代），\

    “验证损失：{:. 3f} ..” \

    .format（dev_losss / iter_2），\

    “训练准确率：{:. 3f} ..” \

    .format（acc / iterations），\

    “验证准确率：{:. 3f}” \

    .format（dev_accs / iter_2））

9.  绘制这两组数据的损失和精度。要绘制损失，请使用以下代码。

    plt.plot（x_axis，train_losses，label ='训练损失'）

    plt.plot（x_axis，dev_losses，label ='验证损失'）

    plt.legend（frameon = False）

    plt.show()

    结果图应类似于以下内容：

    ![Figure 4.23: Resulting plot showing the loss of the sets ](img/B15778_04_23.jpg)

    图 4.23：结果图显示了集合的丢失

    要绘制精度，请使用以下代码：

    plt.plot（x_axis，train_acc，label =“训练精度”）

    plt.plot（x_axis，dev_acc，label =“验证准确率”）

    plt.legend（frameon = False）

    plt.show()

    该图应类似于以下内容：

    ![Figure 4.24: Resulting plot showing the accuracy of the sets ](img/B15778_04_24.jpg)

    图 4.24：结果图显示了集合的准确率

    可以看出，在第 15 个时期之后，过拟合开始影响模型。

10.  在测试集上检查模型的准确性。

    model.eval（）

    iter_3 = 0

    acc_test = 0

    对于 test_loader 中的 data_test 和 target_test：

    iter_3 + = 1

    test_pred =模型（data_test）

    test_pred = torch.exp（test_pred）

    top_p，top_class_test = test_pred.topk（1，暗= 1）

    acc_test + =准确度得分（target_test，top_class_test）

    打印（acc_test / iter_3）

    使用我们之前创建的数据加载器，可以对测试集数据进行图像分类，以估计模型在看不见数据上的准确率。

    如果您的计算机具有可用的 GPU，则对前面的代码进行一些修改，如下所示：

    model.eval（）

    iter_3 = 0

    acc_test = 0

    对于 test_loader 中的 data_test 和 target_test：

    iter_3 + = 1

    test_pred =模型（data_test.to（“ cuda”））

    test_pred = torch.exp（test_pred）

    top_p，top_class_test = test_pred.topk（1，暗= 1）

    acc_test + = precision_score（target_test .to（“ cpu”），\

    top_class_test .to（“ cpu”））

    打印（acc_test / iter_3）

    测试集的准确率与其他两组所达到的准确率非常相似，这意味着该模型能够对看不见的数据表现出同样出色的性能。 它应该在 72% 左右。

    注意

    要访问此特定部分的源代码，请参考[这里](https://packt.live/3gjvWuV)。

    本部分当前没有在线交互示例，需要在本地运行。

    要访问此源代码的 GPU 版本，请参考[这里](https://packt.live/2BUGjGF)。 此版本的源代码无法作为在线交互示例使用，需要通过 GPU 设置在本地运行。

## 活动 4.02：实施数据增强

### 解决方案

1.  复制之前活动中的笔记本。

    为了完成此活动，按照以下步骤，除了修改`tranforms`值之外，不会更改任何代码。

2.  修改`transform`变量的定义，使其除了对数据进行归一化和转换为张量外，还包括以下转换：

    对于训练/验证集，请使用`RandomHorizo​​ntalFlip`函数，其概率为 50% （`0.5`），并使用`RandomGrayscale`函数，其概率为 10% （`0.1`）。

    对于测试集，请勿添加任何其他转换：

    变换= \

    {“火车”：transforms.Compose（[\

    transforms.RandomHorizo​​ntalFlip（0.5），\

    transforms.RandomGrayscale（0.1），\

    transforms.ToTensor（），\

    transforms.Normalize（（0.5，0.5，0.5），\

    (0.5, 0.5, 0.5))]),\

    “ test”：transforms.Compose（[\

    transforms.ToTensor（），\

    transforms.Normalize（（0.5，0.5，0.5），\

    (0.5, 0.5, 0.5))])}

3.  训练模型100个纪元。

    如果您的计算机具有可用的 GPU，请确保使用代码的 GPU 版本来训练模型。

    在训练和验证集上得出的损失和准确率图应与此处显示的图相似：

    ![Figure 4.25: Resulting plot showing the loss of the sets ](img/B15778_04_25.jpg)

    图 4.25：结果图显示了集合的丢失

    ![Figure 4.26: Resulting plot showing the accuracy of the sets ](img/B15778_04_26.jpg)

    图 4.26：结果图显示了集合的准确率

    通过添加数据扩充，可以改善模型的性能，并减少发生的过拟合。

4.  计算所得模型在测试集上的精度。

    该模型在测试设备上的性能提高了约 75%。

    注意

    要访问此特定部分的源代码，请参考[这里](https://packt.live/3ePcAND)。

    本部分当前没有在线交互示例，需要在本地运行。

    要访问此源代码的 GPU 版本，请参考[这里](https://packt.live/38jpq4g)。 此版本的源代码无法作为在线交互示例使用，需要通过 GPU 设置在本地运行。

## 活动 4.03：实现批量标准化

### 解决方案

1.  复制之前活动中的笔记本。

    要完成此活动，按照以下步骤，除了在网络架构中添加一些层之外，不会更改任何代码。

2.  将批量归一化添加到每个卷积层，以及第一个完全连接层。

    网络的最终架构应如下：

    CNN（nn.Module）类：

    def __init __（）：

    超级（净，自我）.__ init __（）

    self.conv1 = nn.Conv2d（3，10，3，1，1）

    self.norm1 = nn.BatchNorm2d（10）

    self.conv2 = nn.Conv2d（10，20，3，1，1）

    self.norm2 = nn.BatchNorm2d（20）

    self.conv3 = nn.Conv2d（20，40，3，1，1）

    self.norm3 = nn.BatchNorm2d（40）

    self.pool = nn.MaxPool2d（2，2）

    self.linear1 = nn.Linear（40 * 4 * 4，100）

    self.norm4 = nn.BatchNorm1d（100）

    self.linear2 = nn.Linear（100，10）

    self.dropout = nn.Dropout（0.2）

    def forward（self，x）：

    x = self.pool（self.norm1（F.relu（self.conv1（x））））

    x = self.pool（self.norm2（F.relu（self.conv2（x））））

    x = self.pool（self.norm3（F.relu（self.conv3（x））））

    x = x.view（-1，40 * 4 * 4）

    x = self.dropout（x）

    x = self.norm4（F.relu（self.linear1（x）））

    x = self.dropout（x）

    x = F.log_softmax（self.linear2（x），dim = 1）

    返回 x

3.  训练模型100个纪元。

    如果您的计算机具有可用的 GPU，请确保使用代码的 GPU 版本来训练模型。 训练和验证集的损失和准确率的结果图应类似于此处所示：

    ![Figure 4.27: Resulting plot showing the loss of the sets ](img/B15778_04_27.jpg)

    图 4.27：结果图显示集合的丢失

    ![Figure 4.28: Resulting plot showing the loss of the sets ](img/B15778_04_28.jpg)

    图 4.28：结果图显示集合的丢失

    尽管过拟合再次引入了模型，但是我们可以看到两组的性能都有所提高。

    注意

    尽管本章未对此进行探讨，但理想的步骤是为网络架构添加辍学以减少高方差。 随意尝试一下，看看您是否能够进一步提高性能。

4.  计算所得模型在测试集上的精度。

    该模型在测试设备上的性能已提高了约 78%。

    注意

    要访问此特定部分的源代码，请参考[这里](https://packt.live/31sSR2G)。

    本部分当前没有在线交互示例，需要在本地运行。

    要访问此源代码的 GPU 版本，请参考[这里](https://packt.live/3eVgp4g)。 此版本的源代码无法作为在线交互示例使用，需要通过 GPU 设置在本地运行。

# 5.样式转移

## 活动 5.01：执行样式转换

### 解决方案

1.  导入所需的库。

    将 numpy 导入为 np

    进口火炬

    从火炬进口 nn，乐观

    从 PIL 导入图片

    导入 matplotlib.pyplot 作为 plt

    从 torchvision 导入转换，模型

    如果您的计算机具有可用的 GPU，请确保定义一个名为`device`的变量，该变量将有助于为 GPU 分配一些变量，如下所示：

    device = "cuda"

2.  指定要对输入图像进行的变换。请确保将它们调整为相同的大小，将它们转换为张力，并将它们标准化。

    尺寸= 224

    加载程序= \

    transforms.Compose（[transforms.Resize（imsize），\

    transforms.ToTensor（），\

    transforms.Normalize（（0.485，0.456，0.406），\

    (0.229, 0.224, 0.225))])

3.  Define an image loader function. It should open the image and load it. Call the image loader function to load both input images:

    def image_loader（图片名称）：

    图片= Image.open（图片名称）

    图片=加载程序（图片）。取消压缩（0）

    返回图片

    content_img = image_loader（“ images / landscape.jpg”）

    style_img = image_loader（“ images / monet.jpg”）

    如果您的计算机有可用的 GPU，请改用以下代码段：

    def image_loader（图片名称）：

    图片= Image.open（图片名称）

    图片=加载程序（图片）。取消压缩（0）

    返回图片

    content_img = image_loader（“ images / landscape.jpg”）。to（device）

    style_img = image_loader（“ images / monet.jpg”）。to（device）

4.  To be able to display the images, set the transformations to revert the normalization of the images and to convert the tensors into **PIL** images:

    卸载程序= transforms.Compose（[\

    transforms.Normalize（（-0.485 / 0.229，\

    -0.456/0.224, \

    -0.406/0.225), \

    (1/0.229, 1/0.224, 1/0.225)),\

    transforms.ToPILImage（）]）

5.  Create a function (**tensor2image**) that's capable of performing the previous transformation over tensors. Call the function for both images and plot the results:

    def tensor2image（张量）：

    图片= tensor.clone（）

    图片= image.squeeze（0）

    图片=卸载程序（图片）

    返回图片

    plt.figure（）

    plt.imshow（tensor2image（content_img））

    plt.title（“内容图片”）

    plt.show()

    plt.figure（）

    plt.imshow（tensor2image（style_img））

    plt.title（“样式图片”）

    plt.show()

    如果您的计算机有可用的 GPU，请改用以下代码段：

    def tensor2image（张量）：

    图片= tensor.to（“ cpu”）。clone（）

    图片= image.squeeze（0）

    图片=卸载程序（图片）

    返回图片

    plt.figure（）

    plt.imshow（tensor2image（content_img））

    plt.title（“内容图片”）

    plt.show()

    plt.figure（）

    plt.imshow（tensor2image（style_img））

    plt.title（“样式图片”）

    plt.show()

6.  Load the VGG-19 model:

    模型= models.vgg19（pretrained = True）。功能

    对于 model.parameters（）中的参数：

    param.requires_grad_（False）

    如果您的计算机有可用的 GPU，请确保将包含模型的变量分配给 GPU，如下所示：

    model.to（设备）

7.  Create a dictionary for mapping the index of the relevant layers (keys) to a name (values). Then, create a function to extract the feature maps of the relevant layers. Use them to extract the features of both input images.

    以下函数应为每个相关层提取给定图像的特征：

    related_layers = {'0'：'conv1_1'，'5'：'conv2_1'，\

    '10'：'conv3_1'，'19'：'conv4_1'，\

    '21'：'conv4_2'，'28'：'conv5_1'}

    def features_extractor（x，模型，层）：

    功能= {}

    用于索引，位于 model._modules.items（）中的层：

    x =层（x）

    如果层索引：

    features [layers [index]] = x

    返回特征

    接下来，应该为**内容**和**样式**图像调用该函数：

    content_features = features_extractor（content_img，\

    型号，

    related_layers）

    style_features = features_extractor（style_img，model，\

    related_layers）

8.  Calculate the gram matrix for the style features. Also, create the initial target image.

    以下代码段为用于提取样式特征的每个层创建了 gram 矩阵：

    style_grams = {}

    对于我在 style_features 中：

    层= style_features [i]

    _，d1，d2，d3 = layer.shape

    功能= layer.view（d1，d2 * d3）

    克= torch.mm（features，features.t（））

    style_grams [i] =克

    接下来，创建初始目标图像作为内容图像的克隆：

    target_img = content_img.clone（）。requires_grad_（正确）

    如果您的计算机有可用的 GPU，请改用以下代码段：

    target_img = content_img.clone（）。\

    require_grad_（True）.to（设备）

9.  Set the weights for different style layers, as well as the weights for the content and style losses:

    style_weights = {'conv1_1'：1.，'conv2_1'：0.8，\

    'conv3_1'：0.6，'conv4_1'：0.4，\

    'conv5_1'：0.2}

    阿尔法= 1

    Beta = 1e5

10.  Run the model for 500 iterations. Define the Adam optimization algorithm before starting to train the model, using`0.001`as the learning rate:

    注意

    为了获得本书所示的最终目标图像，该代码运行了 5,000 次迭代，而没有 GPU 则需要很长时间才能运行。 但是，要欣赏输出图像中开始发生的更改，尽管鼓励您测试不同的训练时间，但只需运行 500 次迭代就足够了。

    print_statement = 500

    优化程序= torch.optim.Adam（[target_img]，lr = 0.001）

    迭代= 5000

    对于范围（1，迭代+1）中的 i：

    ＃提取所有相关层的特征

    target_features = features_extractor（target_img，模型，\

    related_layers）

    ＃计算内容损失

    content_loss = torch.mean（（target_features ['conv4_2'] \

    -content_features ['conv4_2']）** 2）

    ＃遍历所有样式层

    style_losses = 0

    用于 style_weights 中的层：

    ＃为该层创建语法矩阵

    target_feature = target_features [layer]

    _，d1，d2，d3 = target_feature.shape

    target_reshaped = target_feature.view（d1，d2 * d3）

    target_gram = torch.mm（target_reshaped，\

    target_reshaped.t（））

    style_gram = style_grams [layer]

    ＃计算该层的样式损失

    style_loss = style_weights [layer] * \

    torch.mean（（target_gram-\

    style_gram）** 2）

    ＃计算所有层的样式丢失

    style_losses + = style_loss /（d1 * d2 * d3）

    ＃计算总损失

    total_loss = alpha * content_loss + beta * style_losses

    ＃执行反向传播

    Optimizer.zero_grad（）

    total_loss.backward（）

    Optimizer.step（）

    ＃打印目标图像

    如果 i% print_statement == 0 或 i == 1：

    print（'Total loss：'，total_loss.item（））

    plt.imshow（tensor2image（target_img））

    plt.show()

11.  Plot the **content**, **style**, and **target** images to compare the results:

    图，（ax1，ax2，ax3）= plt.subplots（1，3，图大小=（15，5））

    ax1.imshow（tensor2image（content_img））

    ax2.imshow（tensor2image（target_img））

    ax3.imshow（tensor2image（style_img））

    plt.show()

    从此代码段派生的图应类似于此处显示的图：

![Figure 5.11: Output plots ](img/B15778_05_11.jpg)

图 5.11：输出图

注意

要查看 高质量彩色图像，请访问本书的 GitHub 存储库，网址为 https://packt.live/2KcORcw。

要访问此特定部分的源代码，请参考[这里](https://packt.live/2BZj91B)。

本部分当前没有在线交互示例，需要在本地运行。

要访问此源代码的 GPU 版本，请参考[这里](https://packt.live/3eNfvqc)。 此版本的源代码无法作为在线交互示例使用，需要通过 GPU 设置在本地运行。

# 6.使用 RNN 分析数据序列

## 活动 6.01：使用简单的 RNN 进行时间序列预测

### 解决方案

1.  Import the required libraries, as follows:

    将熊猫作为 pd 导入

    导入 matplotlib.pyplot 作为 plt

    进口火炬

    从火炬进口 nn，乐观

2.  Load the dataset and then slice it so that it contains all the rows but only the columns from index 1 to 52:

    数据= pd.read_csv（“ Sales_Transactions_Dataset_Weekly.csv”）

    数据= data.iloc [：，1:53]

    data.head（）

    输出如下：

    ![Figure 6.26: Displaying dataset for columns from index 1 to 52 ](img/B15778_06_26.jpg)

    图 6.26：显示索引 1 到 52 列的数据集

3.  Plot the weekly sales transactions of five randomly chosen products from the entire dataset. Use a random seed of`0`when performing random sampling in order to achieve the same results as in the current activity:

    plot_data = data.sample（5，random_state = 0）

    x =范围（1,53）

    plt.figure（figsize =（10,5））

    对于我，在 plot_data.iterrows（）中行：

    plt.plot（x，行）

    plt.legend（plot_data.index）

    plt.xlabel（“周”）

    plt.ylabel（“每件商品的销售交易”）

    plt.show()

    结果图应如下所示：

    ![Figure 6.27: Plot of the output ](img/B15778_06_27.jpg)

    图 6.27：输出图

4.  Create the **inputs** and **targets** variables that will be fed to the network to create the model. These variables should be of the same shape and be converted into PyTorch tensors.

    `input`变量应包含除上周外的所有星期的所有产品数据，因为模型的目的是预测最后一周。

    `target`变量应比`input`变量领先一步； 也就是说，`target`变量的第一个值应该是输入变量中的第二个，依此类推，直到`target`变量的最后一个值（应该是 被留在`input`变量之外）：

    data_train = data.iloc [：，：-1]

    输入=火炬。张量（data_train.values）。取消挤压（1）

    目标= data_train.shift（-1，axis =“ columns”，\

    fill_value = data.iloc [：，-1]）\

    .astype（dtype =“ float32”）

    目标=火炬。张量（targets.values）

5.  Create a class containing the architecture of the network. Note that the output size of the fully connected layer should be`1`:

    RNN 类（nn.Module）：

    def __init __（self，input_size，hidden_​​size，num_layers）：

    super（）.__ init __（）

    self.hidden_​​size = hidden_​​size

    self.rnn = nn.RNN（input_size，hidden_​​size，\

    num_layers，batch_first = True）

    self.output = nn.Linear（hidden_​​size，1）

    def forward（自我，x，隐藏）：

    out，隐藏= self.rnn（x，隐藏）

    out = out.view（-1，self.hidden_​​size）

    out = self.output（输出）

    返回，隐藏

    与之前的活动一样，该类包含`__init__`方法以及网络架构，以及`forward`方法，该方法确定信息在各层之间的流动。

6.  Instantiate the **class** function containing the model. Feed the input size, the number of neurons in each recurrent layer (`10`), and the number of recurrent layers (`1`):

    模型= RNN（data_train.shape [1]，10，1）

    模型

    运行前面的代码将显示以下输出：

    RNN（

    （rnn）：RNN（51、10，batch_first =真）

    （输出）：线性（in_features = 10，out_features = 1，bias = True）

    )

7.  Define a loss function, an optimization algorithm, and the number of epochs to train the network. Use the MSE loss function, the Adam optimizer, and 10,000 epochs to do this:

    loss_function = nn.MSELoss（）

    优化程序= optim.Adam（model.parameters（），lr = 0.001）

    纪元= 10000

8.  Use a **for** loop to perform the training process by going through all the epochs. In each epoch, a prediction must be made, along with the subsequent calculation of the loss function and the optimization of the parameters of the network. Save the loss of each of the epochs:

    注意

    考虑到没有批量用于遍历数据集，`hidden`量实际上并未在批量之间传递（而是在处理序列的每个元素时使用隐藏状态），但是 为了清楚起见，它留在这里。

    损失= []

    对于范围（1，纪元+1）中的 i：

    隐藏=无

    pred，hidden =模型（输入，隐藏）

    target = target [：，-1] .unsqueeze（1）

    损失= loss_function（目标，pred）

    Optimizer.zero_grad（）

    loss.backward（）

    Optimizer.step（）

    loss.append（loss.item（））

    如果 i% 1000 == 0：

    print（“ epoch：”，i，“ = ...损失函数：”，损失[-1]）

    输出应如下所示：

    时代：1000 ...损失函数：58.48879623413086

    时代：2000 ...损失函数：24.934917449951172

    时代：3000 ...损失函数：13.247632026672363

    时代：4000 ...损失函数：9.884735107421875

    时代：5000 ...损失函数：8.778228759765625

    时代：6000 ...损失函数：8.025042533874512

    时代：7000 ...损失函数：7.622503757476807

    时代：8000 ...损失函数：7.4796295166015625

    时代：9000 ...损失函数：7.351718902587891

    时代：10000 ...损失函数：7.311776161193848

9.  Plot the losses of all epochs, as follows:

    x_range =范围（len（损失））

    plt.plot（x_range，损失）

    plt.xlabel（“时代”）

    plt.ylabel（“损失函数”）

    plt.show()

    结果图应如下所示：

    ![Figure 6.28: Plot displaying the losses of all epochs ](img/B15778_06_28.jpg)

    图 6.28：显示所有时期的损失的图

10.  Using a scatter plot, display the predictions that were obtained in the last epoch of the training process against the ground truth values (that is, the sales transactions of the last week):

    x_range =范围（len（数据））

    目标= data.iloc [：，-1] .values.reshape（len（data），1）

    plt.figure（figsize =（15,5））

    plt.scatter（x_range [：20]，target [：20]）

    plt.scatter（x_range [：20]，pred.detach（）。numpy（）[：20]）

    plt.legend（[[地面真理]，“预测”]）

    plt.xlabel（“产品”）

    plt.ylabel（“销售交易”）

    plt.xticks（范围（0，20））

    plt.show()

    最终图应如下所示：

![Figure 6.29: Scatter plot displaying predictions ](img/B15778_06_29.jpg)

图 6.29：显示预测的散点图

注意

要访问此特定部分的源代码，请参考[这里](https://packt.live/2BqDWvg)。

您也可以通过[这里](https://packt.live/3ihPgKB)在线运行此示例。 您必须执行整个笔记本才能获得所需的结果。

## 活动 6.02：使用 LSTM 网络生成文本

### 解决方案

1.  Import the required libraries, as follows:

    导入数学

    将 numpy 导入为 np

    导入 matplotlib.pyplot 作为 plt

    进口火炬

    从火炬进口 nn，乐观

    导入功能为 F 的 torch.nn。

2.  Open and read the text from *Alice in Wonderland* into the notebook. Print an extract of the first 50 characters and the total length of the text file:

    使用 open（'alice.txt' ，'r'，encoding ='latin1'）作为 f：

    数据= f.read（）

    print（“ Extract：”，data [：50]）

    print（“ Length：”，len（data））

3.  Create a variable containing a list of the unduplicated characters in your dataset. Then, create a dictionary that maps each character to an integer, where the characters will be the keys and the integers will be the values:

    字符=列表（设置（数据））

    indexer = {char：枚举（chars）中（index，char）的索引}

    输出应如下所示：

    摘录：爱丽丝开始对坐 b 感到非常厌倦

    长度：145178

4.  Encode each letter of your dataset to its paired integer. Print the first 50 encoded characters and the total length of the encoded version of your dataset:

    indexed_data = []

    对于 c 在数据中：

    indexed_data.append（indexer [c]）

    print（“ Indexed extract：”，indexed_data [：50]）

    print（“ Length：”，len（indexed_data））

    输出如下：

    索引提取物：[51、52、29、38、28、25、11、59、39、25、16、53、2、26、26、1、26、2、25、56、60、25， 2、53、56、25、23、53、7、45、25、56、1、7、53、13、25、60、14、25、39、1、56、56、1、26、2 25、16]

    长度：145178

5.  Create a function that takes in a batch and encodes it as a one-hot matrix:

    def index2onehot（批量）：

    batch_flatten = batch.flatten（）

    onehot_flat = np.zeros（（（batch.shape [0] \

    * batch.shape [1]，len（indexer）））

    onehot_flat [range（len（batch_flatten）），batch_flatten] = 1

    onehot = onehot_flat.reshape（（batch.shape [0]，\

    batch.shape [1]，-1））

    归还

    此函数采用二维矩阵并将其展平。 接下来，它创建一个平坦矩阵的形状和包含字母的字典长度的零填充矩阵（在“步骤 3”中创建）。 接下来，它用一个字符填充对应于批量中每个字符的字母。 最后，它对矩阵进行整形以使其为三维。

6.  Create a class that defines the architecture of the network. This class should contain an additional function that initializes the states of the LSTM layers:

    LSTM（nn.Module）类：

    def __init __（self，char_length，hidden_​​size，n_layers）：

    super（）.__ init __（）

    self.hidden_​​size = hidden_​​size

    self.n_layers = n_layers

    self.lstm = nn.LSTM（char_length，hidden_​​size，\

    n_layers，batch_first = True）

    self.output = nn.Linear（hidden_​​size，char_length）

    def forward（自我，x，状态）：

    出，状态= self.lstm（x，状态）

    out = out.contiguous（）。view（-1，self.hidden_​​size）

    out = self.output（输出）

    返回状态

    def init_states（self，batch_size）：

    隐藏= next（self.parameters（））\

    .data.new（self.n_layers，batch_size，\

    self.hidden_​​size）.zero_（）

    单元格= next（self.parameters（））\

    .data.new（self.n_layers，batch_size，\

    self.hidden_​​size）.zero_（）

    状态=（隐藏的单元格）

    返回状态

    此类包含`__init__`方法（其中定义了网络的架构），`forward`方法（用于确定通过层的数据流）以及`init_state`用零初始化隐藏状态和单元状态的方法。

7.  Determine the number of batches to be created out of your dataset, bearing in mind that each batch should contain 100 sequences, each with a length of 50\. Next, split the encoded data into 100 sequences:

    ＃每批序列​​数

    n_seq = 100

    seq_length = 50

    n_batches = math.floor（len（indexed_data）\

    / n_seq / seq_length）

    total_length = n_seq * seq_length * n_batches

    x = indexed_data [：total_length]

    x = np.array（x）.reshape（（[ n_seq，-1））

8.  Instantiate your model by using`256`as the number of hidden units for a total of two recurrent layers:

    模型= LSTM（仅（字符），256、2）

    模型

    运行前面的代码将显示以下输出：

    LSTM（

    （lstm）：LSTM（70，256，num_layers = 2，batch_first = True）

    （输出）：线性（in_features = 256，out_features = 70，bias = True）

    )

    如果您的计算机有可用的 GPU，请确保使用以下代码片段将模型分配给 GPU：

    模型= LSTM（len（chars），256，2）.to（“ cuda”）

9.  Define the loss function and the optimization algorithms. Use the Adam optimizer and the cross-entropy loss to do this. Train the network for`20`epochs:

    loss_function = nn.CrossEntropyLoss（）

    优化程序= optim.Adam（model.parameters（），lr = 0.001）

    时代= 20

    如果您的机器有可用的 GPU，请尝试运行`500`时期的训练过程：

    时代= 500

10.  In each epoch, the data must be divided into batches with a sequence length of 50\. This means that each epoch will have 100 batches, each with a sequence of 50:

    损失= []

    对于范围（1，历元+1）中的 e：

    状态= model.init_states（n_seq）

    batch_loss = []

    对于范围为（0，x .shape [1]，seq_length）的 b：

    x_batch = x [：，b：b + seq_length]

    如果 b == x.shape [1]-seq_length：

    y_batch = x [：，b + 1：b + seq_length]

    y_batch = np.hstack（（y_batch，indexer [“。”] \

    * np.ones（（y_batch.shape [0]，1））））

    其他：

    y_batch = x [：，b + 1：b + seq_length + 1]

    x_onehot =火炬张量（index2onehot（x_batch））

    y =火炬。张量（y_batch）.view（n_seq * seq_length）

    pred，状态=模型（x_onehot，状态）

    损失= loss_function（pred，y.long（））

    Optimizer.zero_grad（）

    loss.backward（retain_graph = True）

    Optimizer.step（）

    batch_loss.append（loss.item（））

    loss.append（np.mean（batch_loss））

    如果 e% 2 == 0：

    print（“ epoch：”，e，“ ...损失函数：”，损失[-1]）

    输出应如下所示：

    时代：2 ...损失函数：3.1667490992052802

    时代：4 ...损失函数：3.1473221943296235

    时期：6 ...损失函数：2.897721455014985

    时代：8 ...损失函数：2.567064647016854

    时代：10 ...损失函数：2.4197753791151375

    时期：12 ...损失函数：2.314083896834275

    时代：14 ...损失函数：2.2241266349266313

    时期：16 ...损失函数：2.1459227183769487

    时代：18 ...损失函数：2.0731402758894295

    时代：20 ...损失函数：2.0148646708192497

    如果您的计算机具有可用的 GPU，则用于训练网络的等效代码段如下所示：

    损失= []

    对于范围（1，历元+1）中的 e：

    状态= model.init_states（n_seq）

    batch_loss = []

    对于范围为（0，x.shape [1]，seq_length）的 b：

    x_batch = x [：，b：b + seq_length]

    如果 b == x.shape [1]-seq_length：

    y_batch = x [：，b + 1：b + seq_length]

    y_batch = np.hstack（（y_batch，indexer [“。”] \

    * np.ones（（y_batch.shape [0]，1））））

    其他：

    y_batch = x [：，b + 1：b + seq_length + 1]

    x_onehot =火炬张量（index2onehot（x_batch））\

    .to("cuda")

    y =火炬。张量（y_batch）.view（n_seq * \

    seq_length）.to（“ cuda”）

    pred，状态=模型（x_onehot，状态）

    损失= loss_function（pred，y.long（））

    Optimizer.zero_grad（）

    loss.backward（retain_graph = True）

    Optimizer.step（）

    batch_loss.append（loss.item（））

    loss.append（np.mean（batch_loss））

    如果 e% 50 == 0：

    print（“ epoch：”，e，“ ...损失函数：”，\

    损失[-1]）

    将训练过程运行 500 个纪元的结果如下：

    时代：50 ...损失函数：1.5207843986050835

    时代：100 ...损失函数：1.006190665836992

    时代：150 ...损失函数：0.5197970939093622

    时期：200 ...损失函数：0.24446514968214364

    时期：250 ...损失函数：0.0640328845073437

    时期：300 ...损失函数：0.007852113484565553

    时期：350 ...损失函数：0.003644719101681278

    时代：400 ...损失函数：0.006955199634078248

    时代：450 ...损失函数：0.0030021724242973945

    时代：500 ...损失函数：0.0034294885518992768

    可以看出，通过将训练过程运行更多的时间段，损失函数将达到较低的值。

11.  Plot the progress of the loss over time:

    x_range =范围（len（损失））

    plt.plot（x_range，损失）

    plt.xlabel（“时代”）

    plt.ylabel（“损失函数”）

    plt.show()

    该图表应如下所示：

    ![Figure 6.30: Chart displaying the progress of the loss function ](img/B15778_06_30.jpg)

    图 6.30：显示损失函数进度的图表

    如我们所见，在 20 个历元之后，损失函数仍然可以减少，这就是为什么强烈建议训练更多历元以便从模型中获得良好结果的原因。

12.  Feed the following sentence **starter** into the trained model for it to complete the sentence: **"So she was considering in her own mind "**:

    starter =“所以她在 中考虑自己的想法”

    状态=无

    如果您的计算机具有可用的 GPU，则将模型分配回 CPU 以执行预测：

    型号= model.to（“ cpu”）

    首先，`for`循环的将种子输入模型，以便可以生成内存。 接下来，执行预测，如以下代码片段所示：

    对于启动器中的 ch：

    x = np.array（[[indexer [ch]]]）

    x = index2onehot（x）

    x =火炬张量（x）

    pred，状态=模型（x，状态）

    计数器= 0

    而 starter [-1]！=“ 且计数器<100：

    计数器+ = 1

    x = np.array（[[indexer [starter [-1]]]]）

    x = index2onehot（x）

    x =火炬张量（x）

    pred，状态=模型（x，状态）

    pred = F.softmax（pred，dim = 1）

    p，上层=上一层（10）

    p = p.detach（）。numpy（）[0]

    top = top.numpy（）[0]

    索引= np.random.choice（top，p = p / p.sum（））

    入门级+ =字符[索引]

    打印（入门）

    注意

    要访问此特定部分的源代码，请参考[这里](https://packt.live/2Bs6dRZ)。

    本部分当前没有在线交互示例，需要在本地运行。

    要访问此源代码的 GPU 版本，请参考[这里](https://packt.live/3g9X6UI)。 此版本的源代码无法作为在线交互示例使用，需要通过 GPU 设置在本地运行。

## 活动 6.03：执行情感分析的 NLP

### 解决方案

1.  导入所需的库。

    将熊猫作为 pd 导入

    将 numpy 导入为 np

    导入 matplotlib.pyplot 作为 plt

    从字符串导入标点

    从 sklearn.metrics 导入 precision_score

    进口火炬

    从火炬进口 nn，乐观

    导入功能为 F 的 torch.nn。

2.  Load the dataset containing a set of 1,000 product reviews from Amazon, which is paired with a label of`0`(for negative reviews) or`1`(for positive reviews). Separate the data into two variables – one containing the reviews and the other containing the labels:

    数据= pd.read_csv（“ amazon_cells_labelled.txt”，sep =“ \ t”，\

    标头=无）

    评论= data.iloc [：，0] .str.lower（）

    情绪= data.iloc [：，1] .values

3.  Remove the punctuation from the reviews:

    对于我的标点符号：

    评论= reviews.str.replace（i，“”）

4.  Create a variable containing the vocabulary of the entire set of reviews. Additionally, create a dictionary that maps each word to an integer, where the words will be the keys and the integers will be the values:

    words =''.join（评论）

    单词= words.split（）

    词汇=设置（单词）

    indexer = {word ：（索引，单词）的索引\

    枚举（词汇）}

5.  Encode the reviews data by replacing each word in a review with its paired integer:

    indexed_reviews = []

    在评论中进行评论：

    indexed_reviews.append（[indexer [word] \

    for review.split（）中的单词）

6.  Create a class containing the architecture of the network. Make sure that you include an embedding layer:

    LSTM（nn.Module）类：

    def __init __（self，vocab_size，embed_dim，\

    hidden_​​size，n_layers）：

    super（）.__ init __（）

    self.hidden_​​size = hidden_​​size

    self.embedding = nn.Embedding（vocab_size，embed_dim）

    self.lstm = nn.LSTM（embed_dim，hidden_​​size，\

    n_layers，batch_first = True）

    self.output = nn.Linear（hidden_​​size，1）

    def forward（self，x）：

    出= self.embedding（x）

    出，_ = self.lstm（出）

    out = out.contiguous（）。view（-1，self.hidden_​​size）

    out = self.output（输出）

    出=出[-1,0]

    out =火炬.sigmoid（out）.unsqueeze（0）

    返回

    该类包含用于定义网络架构的`__init__`方法和用于确定数据流经不同层的方式的`forward`方法。

7.  Instantiate the model using 64 embedding dimensions and 128 neurons for three LSTM layers:

    模型= LSTM（仅（词汇），64、128、3）

    模型

    运行前面的代码将显示以下输出：

    LSTM（

    （嵌入）：嵌入（1905，64）

    （lstm）：LSTM（64、128，num_layers = 3，batch_first = True）

    （输出）：线性（in_features = 128，out_features = 1，bias = True）

    )

8.  Define the loss function, an optimization algorithm, and the number of epochs to train for. For example, you can use the binary cross-entropy loss as the loss function, the Adam optimizer, and train for 10 epochs:

    loss_function = nn.BCELoss（）

    优化程序= optim.Adam（model.parameters（），lr = 0.001）

    时代= 10

9.  Create a **for** loop that goes through the different epochs and through every single review individually. For each review, perform a prediction, calculate the loss function, and update the parameters of the network. Additionally, calculate the accuracy of the network on that training data:

    损失= []

    acc = []

    对于范围（1，历元+1）中的 e：

    single_loss = []

    preds = []

    目标= []

    对于我来说，r 在枚举（indexed_reviews）中：

    如果 len（r）<= 1：

    继续

    x =火炬.Tensor（[r]）。long（）

    y = Torch.Tensor（[sentiment [i]]）

    之前=模型（x）

    损失= loss_function（pred，y）

    Optimizer.zero_grad（）

    loss.backward（）

    Optimizer.step（）

    final_pred = np.round（pred.detach（）。numpy（））

    preds.append（final_pred）

    target.append（y）

    single_loss.append（loss.item（））

    loss.append（np.mean（single_loss））

    精度=精度得分（目标，优势）

    追加（精度）

    如果 e% 1 == 0：

    print（“ Epoch：”，e，“ ...损失函数：”，损失[-1]，\

    “ ...精度：”，acc [-1]）

    与以前的活动一样，训练过程包括进行预测，将其与基本事实进行比较以计算损失函数，并执行向后传递以最小化损失函数。

10.  Plot the progress of the loss and accuracy over time. The following code is used to plot the loss function:

    x_range =范围（len（损失））

    plt.plot（x_range，损失）

    plt.xlabel（“时代”）

    plt.ylabel（“损失函数”）

    plt.show()

    该图应如下所示：

![Figure 6.31: Plot displaying the progress of the loss function  ](img/B15778_06_31.jpg)

图 6.31：显示损失函数进度的图

以下代码用于绘制准确率得分：

x_range =范围（len（acc））

plt.plot（x_range，acc）

plt.xlabel（“时代”）

plt.ylabel（“准确率得分”）

plt.show()

该图应如下所示：

![Figure 6.32: Plot displaying the progress of the accuracy score ](img/B15778_06_32.jpg)

图 6.32：显示准确率得分进度的图

注意

要访问此特定部分的源代码，请参考[这里](https://packt.live/2VyX0ON)。

本部分当前没有在线交互示例，需要在本地运行。