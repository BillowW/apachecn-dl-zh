# 马尔可夫决策过程

**马尔可夫决策过程**（通常称为 **MDP**）是一种强化学习的方法，可用于在网格世界环境中进行决策。 Gridworld 环境由网格形式的状态组成，例如 OpenAI Gym 的 FrozenLake-v0 环境中的状态，我们在上一章中试图进行研究和解决。

MDP 试图通过将网格划分为状态，动作，模型/转换模型和奖励来捕获网格形式的世界。 MDP 的解决方案称为策略，目标是为该 MDP 任务找到最佳策略。

因此，任何遵循 Markov 属性的，由一组状态，动作和奖励组成的强化学习任务都将被视为 MDP。

在本章中，我们将深入研究 MDP，状态，动作，奖励，策略，以及如何使用贝尔曼方程求解它们。 此外，我们将介绍部分可观察的 MDP 的基础知识及其解决的复杂性。 我们还将介绍探索利用难题和著名的 E3（开发，探索或利用）算法。 然后我们将进入引人入胜的部分，在该部分中，我们将为代理编程，以使用 MDP 原理学习和玩乒乓球。

我们将在本章介绍以下主题：

*   马尔可夫决策过程
*   部分可观察的马尔可夫决策过程
*   使用 MDP 训练 FrozenLake-v0 环境

# 马尔可夫决策过程

如前所述，MDP 是网格世界环境中的强化学习方法，其中包含状态，动作和奖励集，遵循马尔可夫属性以获得最佳策略。 MDP 被定义为以下各项的集合：

*   **状态**：`S`
*   **动作**：`A(s), A`
*   **转移模型**：`T(s, a, s') ~ P(s' | s, a)`
*   **奖励**：`R(s), R(s, a), R(s, a, s')`
*   **策略**：`π(s) -> a[π*]`是最佳策略

对于 MDP，环境是完全可观察的，也就是说，代理在任何时间点所做的任何观察都足以做出最佳决策。 在部分可观察的环境中，代理程序需要一个内存来存储过去的观察结果，以便做出最佳决策。

让我们尝试将其分解为不同的乐高积木，以了解整个过程的含义。

# 马尔可夫属性

简而言之，根据**马尔可夫属性**，为了了解不久的将来的信息（例如，在时间`t + 1`），当前信息在时间`t`很重要。

给定序列`[x[1], ..., x[t]]`，马尔可夫一阶表示`P(x[t] | x[t-1], ..., x[1]) = P(x[t] | x[t-1])`，即`x[t]`仅取决于`x[t-1]`。 因此，`x[t+1]`将仅取决于`x[t]`。 马尔可夫的二阶表示`P(x[t] | x[t-1], ..., x[1]) = P(x[t] | x[t-1], x[t-2])`，即`x[t]`仅取决于`x[t-1]`和`x[t-2]`。

在我们的上下文中，从现在开始，我们将遵循马尔科夫属性的一阶。 因此，如果新状态的概率`x[t+1]`仅取决于当前状态`x[t]`，则我们可以将任何过程转换为马尔科夫属性，以便当前状态捕获并记住过去的属性和知识 。 因此，根据马尔可夫属性，世界（即环境）被认为是固定的，也就是说，世界中的规则是固定的。

# `S`状态集

**`S`状态集**是构成环境的一组不同状态，表示为`s`。 状态是从环境获得的数据的特征表示。 因此，来自代理传感器的任何输入都可以在状态形成中发挥重要作用。 状态空间可以是离散的也可以是连续的。 从开始状态开始，必须以最优化的路径达到目标状态，而不会在不良状态下结束（例如下图所示的红色状态）。

考虑以下网格世界，它具有 12 个离散状态，其中绿色网格是目标状态，红色是要避免的状态，黑色是一面碰到墙壁就会反弹的墙壁：

![](img/8a30501f-c8d5-4a0c-a3a0-043d4829fb03.png)

状态可以表示为 1、2，.....，12 或由坐标`(1, 1)`，`(1, 2)`，.....`(3, 4)`表示。

# 动作

**动作**是代理可以在特定状态下执行或执行的操作。 换句话说，动作是允许代理在给定环境中执行的一组操作。 像状态一样，动作也可以是离散的或连续的。

考虑以下具有 12 个离散状态和 4 个离散动作（`UP`，`DOWN`，`RIGHT`和`LEFT`）的 gridworld 示例：

![](img/0afe050d-5e8b-4dbf-82c0-be526f469140.png)

前面的示例将动作空间显示为离散的设置空间，即`a ∈ A`，其中`A = {UP, DOWN, LEFT, RIGHT}`。 也可以将其视为状态的函数，即`a = A(s)`，其中根据状态函数来确定可能采取的行动。

# 转移模型

转移模型`T(s, a, s')`是三个变量的函数，它们是当前状态（`s`），动作（`a`） 以及新状态（`s'`），并定义了在环境中玩游戏的规则。 它给出了`P(s' | s, a)`的概率，也就是说，假设代理采取了行动`a`，在给定状态`s`下降落到新的`s'`状态的概率。

转移模型在随机世界中起着至关重要的作用，与确定性世界的情况不同，在确定性世界中，除了确定的着陆状态之外，任何着陆状态的概率都为零。

让我们考虑以下环境（世界），并考虑确定和随机的不同情况：

![](img/86a85406-eeaf-450f-b93b-b11d8d1f3283.png)

由于动作`a ∈ A`，其中`A = {UP, DOWN, LEFT, RIGHT}`。

这两种情况的行为取决于某些因素：

*   **确定的环境**：在确定的环境中，如果您采取某种行动，例如说`UP`，则您肯定会以概率 1 执行该行动。
*   **随机环境**：在随机环境中，如果您执行相同的操作（例如说`UP`），则有一定的概率说 0.8 可以实际执行给定的操作，而有 0.1 的概率可以执行垂直于给定`UP`动作的动作（`LEFT`或`RIGHT`）。 在此，对于`s`状态和`UP`动作转换模型，`T(s', UP, s) = P(s' | s, UP) = 0.8`。

由于`T(s, a, s') ~ P(s' | s, a)`，因此新状态的概率仅取决于当前状态和操作，而与过去状态无关。 因此，转移模型遵循一阶马尔可夫性质。

我们也可以说我们的宇宙也是一个随机环境，因为宇宙是由处于不同状态的原子组成的，这些原子由位置和速度定义。 每个原子执行的动作都会改变其状态，并导致宇宙发生变化。

# 奖励

状态的**奖励**量化了进入状态的有用性。 有三种不同的形式来表示奖励，即`R(s)`，`R(s, a)`和`R(s, a, s')`，但它们都是等效的。

对于特定环境，领域知识在不同状态的奖励分配中起着重要作用，因为奖励中的细微变化对于找到 MDP 问题的最佳解决方案确实很重要。

当采取某种行动时，我们会奖励代理商以两种方式。 他们是：

*   **信用分配问题**：我们回顾过去，检查哪些活动导致了当前的奖励，即哪个活动获得了荣誉
*   **延迟奖励**：相比之下，在当前状态下，我们检查要采取的行动会导致我们获得潜在的奖励

延迟的报酬形成了远见规划的想法。 因此，该概念被用于计算不同状态的预期奖励。 我们将在后面的部分中对此进行讨论。

# 策略

到现在为止，我们已经解决了造成 MDP 问题的块，即状态，动作，转移模型和奖励，现在是解决方案。 该策略是解决 MDP 问题的方法。

![](img/ec5c8b7c-4b92-426c-97e0-673286364a05.png)

策略是将状态作为输入并输出要采取的措施的功能。 因此，该策略是代理必须遵守的命令。

![](img/8b92a7d2-aa9e-465a-88d3-7880d412516e.png)被称为最佳策略，它使预期收益最大化。 在采取的所有策略中，最优策略是一种优化的策略，用于最大化一生中已收到或预期收到的奖励金额。 对于 MDP，生命周期没有尽头，您必须确定结束时间。

因此，该策略不过是一个指南，它告诉您针对给定状态应采取的行动。 它不是计划，而是通过返回针对每个状态要采取的行动来揭示环境的基础计划。

# 奖励序列-假设

奖励序列在找到 MDP 问题的最佳策略中起着重要作用，但是有一些假设揭示了一系列奖励如何实现延迟奖励的概念。

# 无限的视野

第一个假设是无限的视野，即从起始状态到达目标状态的无限时间步长。 因此，

![](img/b8a7da56-da61-4a36-a9ff-098fc89c0451.png)

策略功能未考虑剩余时间。 如果是有限期的话，那么策略应该是

![](img/4eecac99-2630-4813-9c1f-df9712f7780b.png)

其中`t`是完成任务所需的时间。

因此，如果没有无限远景的假设，那么策略的概念就不会是固定不变的，即`π(s) -> a`，而是`π(s, t) -> a`。

# 序列的效用

序列的效用是指当代理人经历状态序列时所获得的总报酬。 它表示为`U(s[0], s[1], s[2], ...)`，其中`s[0], s[1], s[2], ...`表示状态序列。

第二个假设是，如果有两个效用`U(s[0], s[1], s[2], ...)`和`U(s[0], s'[1], s'[2], ...)`，则两个序列的开始状态都相同，并且

![](img/bb59d758-3f56-4513-bc9d-9d1d8292d013.png)

然后，

![](img/0abf959d-3da4-4b0a-9ad2-af3631dd0381.png)

这意味着，如果序列`U(s[0], s[1], s[2], ...)`的效用大于另一个函数`U(s[0], s'[1], s'[2], ...)`，只要两个序列的起始状态都相同，那么没有该起始状态的序列将具有相同的不等式，即`U(s[1], s[2], ...)`将大于`U(s'[1], s'[2], ...)`。 这种假设称为偏好的平稳性。

因此，以下等式满足了偏好的平稳性，

![](img/9d4cca6c-6221-43c3-98d4-b310017f62cf.png)

上式中的求和足以满足我们的假设，但它有两个缺点，如下所示：

*   无限的时间将使求和无限
*   求和的大小写或顺序无异，即`U(a, b, c)`和`U(a, c, b)`的效用值相同，即`R(a) + R(b) + R(c)`

因此，我们采用折扣系数`γ`来实现未来奖励的概念，即延迟奖励。

![](img/82d34b1d-de39-4e24-99d9-7e440d9b03b4.png)

其中`0 <= γ < 1`。

让我们考虑所有`R(s[t])`，即在给定特定环境中来自不同状态的奖励，`R_max`是最大值，然后

![](img/108100d2-dded-4635-afa3-4246591385cb.png)

怎么样？ 让我们算出这个上限，

由于`R(s[t]) <= R_max`，

因此，`γ^t R(s[t]) <= γ^t R_max`，

因此，

![](img/64493a99-de50-43fc-adb9-7369d0d0ae8e.png)

让，

![](img/ac4374e7-cbfa-4d55-a3f8-d55cac56d8e2.png)

然后，

![](img/fe553776-03a4-41f5-8eda-01ccfa694b7a.png)

从而，

![](img/8f07a082-d0fe-467d-9593-d0ecc0a1e2b6.png)

因此，

![](img/8f5b159e-922b-4faf-bdac-df4b213f271e.png)

# 贝尔曼方程

由于最佳`π*`策略是使预期收益最大化的策略，因此，

![](img/2ccc6145-78d9-4ba3-a12e-19b454d98f69.png)，

其中`E[...]`表示从状态序列获得的报酬的期望值，代理观察其是否遵循`π`策略。 因此，`argmax[π]`输出具有最高预期奖励的`π`策略。

同样，我们也可以计算状态为的策略效用，也就是说，如果我们处于`s`状态，则给定`π`策略，则， `s`状态的`π`策略，即`U[π](s)`将是从该状态开始的预期收益：

![](img/c99b8ca1-0cfb-419a-bc57-c9de97773b37.png)

由于延迟奖励的概念，状态的立即奖励`R(s)`与状态的效用`U(s)`（即状态的最佳策略的效用`U[π*](s)`）不同。 从现在开始，状态的效用`U(s)`将指该状态的最佳策略的效用（即`U[π*](s)`）。

此外，最佳策略也可以被视为最大化预期效用的策略。 因此，

![](img/c1a7f9d8-d174-4168-b470-01cde9c8ec20.png)

其中，`T(s, a, s')`是转移概率，即`P(s' | s, a)`；在对`s`状态采取`a`动作后，`U(s')`是新着陆状态的效用。

`Σ[s'] T(s, a, s')U(s')`指的是针对特定行动采取的所有可能的新状态结果的总和，然后无论哪个行动给出了`Σ[s'] T(s, a, s')U(s')`的最大值，该最大值被认为是最优策略的一部分，因此， 状态由以下 **贝尔曼方程**给出，

![](img/0ef93e7c-5cc5-45ab-8baf-db2a07bd9e29.png)

其中，`R(s)`是即时奖励，`max[a] Σ[s'] T(s, a, s')U(s')`是来自未来的奖励，也就是说，如果采取行动`a`，代理可以从给定的`s`状态到达的`s'`状态的贴现效用。

# 解决贝尔曼方程来找到策略

假设在给定的环境中我们有`n`个状态，如果我们看到贝尔曼方程，

![](img/5051739f-0788-416d-9182-38ae2169ffca.png)

我们发现给出了`n`个状态； 因此，我们将拥有`n`方程，而`n`未知，但是`max[a]`函数使其变为非线性。 因此，我们不能将它们作为线性方程式求解。

因此，为了解决：

*   从任意效用开始

*   根据邻域更新效用，直到收敛为止，也就是说，根据给定状态的着陆状态的效用，使用贝尔曼方程更新状态的效用

多次迭代以得出状态的真实值。 迭代以收敛到状态的真实值的过程称为**值迭代**。

对于游戏结束的终端状态，那些终端状态的效用等于代理在进入终端状态时所获得的立即奖励。

让我们尝试通过示例来理解这一点。

# 使用贝尔曼方程的值迭代的示例

请考虑以下环境和给定的信息：

![](img/9115f073-9e8f-497e-9d96-c52a668dbef4.png)

给定信息：

*  `A`，`C`和`X`是某些状态的名称。

*   绿色状态是目标状态`G`，奖励为 +1。

*   红色状态为错误状态`B`，奖励为 -1，请尝试阻止您的代理进入此状态

*   因此，绿色和红色状态是终端状态，进入其中一个状态，游戏结束。 如果座席遇到绿色状态（即进球状态），则座席获胜；如果他们进入红色状态，则座席将输掉比赛。

*   `γ = 1/2`，`R(s) = -0.04`（即，对除`G`和`B`状态之外的所有状态的奖励是 -0.04），`U[0](s) = 0`（即，第一个时间步长的效用为 0，除`G`和`B`状态外）。

*   如果沿期望的方向前进，则过渡概率`T(s, a, s')`等于 0.8； 否则，如果垂直于所需方向，则各为 0.1。 例如，如果操作是`UP`，则概率为 0.8，代理会`UP`，但概率为 0.1 时，它会`RIGHT`，而 0.1 则为`LEFT`。

问题：

1.  找到`U[1](X)`，时间步骤 1 的`X`状态的效用，也就是说，代理将进行一次迭代
2.  同样，找到`U[2](X)`

解：

![](img/8ec27bf9-be88-4e5e-83e6-eb42a58b57b8.png)

![](img/827fc525-4ce3-4ba0-8b43-6f8abfbc5592.png)

`R(X) = -0.04`

| **动作** | `s'` | ![](img/6fd891e0-ce75-462e-b4eb-a59aed75ce93.png) | ![](img/22ac4a64-43bd-401c-92b7-a91376a60576.png) | ![](img/8795d96c-8164-4074-a370-586d0a1ff9a0.png) |
| --- | --- | --- |
| `RIGHT` | G | 0.8 | +1 | `0.8 x 1 = 0.8` |
| `RIGHT` | C | 0.1 | 0 | `0.1 x 0 = 0` |
| `RIGHT` | X | 0.1 | 0 | `0.1 x 0 = 0` |

因此，对于动作`a = RIGHT`，

![](img/42781834-798e-4673-a3ac-1b5601553691.png)

| **动作** | `s'` | ![](img/001a79e8-232c-416c-b1e7-c8d89cb12a90.png) | ![](img/9a391f2f-337e-46c8-b314-7be8f2e1effa.png) | ![](img/8eb44a6d-f977-4086-b239-2e592f166cbf.png) |
| --- | --- | --- |
| `DOWN` | C | 0.8 | 0 | `0.8 x 0 = 0` |
| `DOWN` | G | 0.1 | +1 | `0.1 x 1 = 0.1` |
| `DOWN` | A | 0.1 | 0 | `0.1 x 0 = 0` |

因此，对于动作`a = DOWN`，

![](img/77918404-8e8e-4efa-9b97-1001d8bc8f4c.png)

| **动作** | `s'` | ![](img/acc2cb11-44c3-4325-9daa-68fe740d67cd.png) | ![](img/b16f854f-055d-4ad4-898f-d6a84b08a775.png) | ![](img/55c7b3a8-f1c9-4e3f-ba20-69dcb80d4244.png) |
| --- | --- | --- |
| `UP` | X | 0.8 | 0 | `0.8 x 0 = 0` |
| `UP` | G | 0.1 | +1 | `0.1 x 1 = 0.1` |
| `UP` | A | 0.1 | 0 | `0.1 x 0 = 0` |

因此，对于动作`a = UP`，

![](img/98b807be-a36f-472a-97d1-0dad9bf4f92e.png)

| **动作** | `s'` | ![](img/782bf5ec-3014-40c2-803d-dd2163340c53.png) | ![](img/90de6f01-fb3e-4558-b557-520defc86bab.png) | ![](img/7817ee4f-64fd-44ba-b2c4-7ad635cea69e.png) |
| --- | --- | --- |
| `LEFT` | A | 0.8 | 0 | `0.8 x 0 = 0` |
| `LEFT` | X | 0.1 | 0 | `0.1 x 0 = 0` |
| `LEFT` | C | 0.1 | 0 | `0.1 x 0 = 0` |

因此，对于动作`a = LEFT`，

![](img/9813b6c4-66cf-4dac-8764-0583ad1fff5f.png)

因此，在所有动作中

![](img/393b0f69-fd8a-4298-8b2a-cb478a41e930.png)

因此，`U[1](X) = -0.04 + 0.5 * 0.8 = 0.36`，其中`R(X) = -0.04`和`γ = 1/2 = 0.5`。

类似地，计算`U[1](A)`和`U[1](C)`，我们得到``R(X) = -0.04``和`γ = 1/2 = 0.5`。

由于`U[1](X) = 0.36, U[1](A) = -0.04, U[1](C) = -0.04, U[1](G) = 1, U[1](B) = -1`，

![](img/bc2a8cef-a5a1-439c-a602-9d309920ca8d.png)

`R(X) = -0.04`

| **动作** | `s'` | ![](img/7eab76d9-3bb5-4e5c-baf6-76ac3ab4e7d7.png) | ![](img/22ac4a64-43bd-401c-92b7-a91376a60576.png) | ![](img/8795d96c-8164-4074-a370-586d0a1ff9a0.png) |
| --- | --- | --- | --- | --- |
| `RIGHT` | G | 0.8 | +1 | `0.8 x 1 = 0.8` |
| `RIGHT` | C | 0.1 | -0.04 | `0.1 x -0.04 = -0.004` |
| `RIGHT` | X | 0.1 | 0.36 | `0.1 x 0.36 = 0.036` |

因此，对于动作`a = RIGHT`，

![](img/8eabbfa6-ba45-4387-82fd-804d44792a6d.png)

| **动作** | `s'` | ![](img/59516b7d-b5b9-4309-abff-a42760a70dbd.png) | ![](img/9a391f2f-337e-46c8-b314-7be8f2e1effa.png) | ![](img/8eb44a6d-f977-4086-b239-2e592f166cbf.png) |
| --- | --- | --- | --- | --- |
| `DOWN` | C | 0.8 | -0.04 | `0.8 x -0.04 = -0.032` |
| `DOWN` | G | 0.1 | +1 | `0.1 x 1 = 0.1` |
| `DOWN` | A | 0.1 | -0.04 | `0.1 x -0.04 = -0.004` |

因此，对于动作`a = DOWN`，

![](img/bf1e83c6-52e3-46d9-91fb-6be453900357.png)

| **动作** | `s'` | ![](img/aeb2ed22-fa4e-4c2d-9a9d-247d56150c02.png) | ![](img/b16f854f-055d-4ad4-898f-d6a84b08a775.png) | ![](img/55c7b3a8-f1c9-4e3f-ba20-69dcb80d4244.png) |
| --- | --- | --- | --- | --- |
| `UP` | X | 0.8 | 0.36 | `0.8 x 0.36 = 0.288` |
| `UP` | G | 0.1 | +1 | `0.1 x 1 = 0.1` |
| `UP` | 一种 | 0.1 | -0.04 | `0.1 x -0.04 = -0.004` |

因此，对于动作`a = UP`，

![](img/a1393b1e-af2c-4d45-a9d3-9f939826c082.png)

| **动作** | `s'` | ![](img/cafa86c3-0a4d-46b0-ade6-eb54a20d65f5.png) | ![](img/90de6f01-fb3e-4558-b557-520defc86bab.png) | ![](img/7817ee4f-64fd-44ba-b2c4-7ad635cea69e.png) |
| --- | --- | --- | --- | --- |
| `LEFT` | 一种 | 0.8 | -0.04 | `0.8 x -0.04 = -0.032` |
| `LEFT` | X | 0.1 | 0.36 | `0.1 x 0.36 = 0.036` |
| `LEFT` | C | 0.1 | -0.04 | `0.1 x -0.04 = -0.004` |

因此，对于动作`a = LEFT`，

![](img/6b4a7765-353e-41bd-9d1d-b0d911f5fcb8.png)

因此，在所有动作中

![](img/7b5c01bd-c97a-4b98-a1ca-2b518675d6bd.png)

因此，`U[2](X) = -0.04 + 0.5 * 0.832 = 0.376`，其中`R(X) = -0.04`和`γ = 1/2 = 0.5`。

因此，上述问题的答案是：

1.  `U[1](X) = 0.36`
2.  `U[2](X) = 0.376`

# 策略迭代

通过迭代策略并更新策略本身（而不是价值）直到策略收敛到最优值以获得最优效用的过程称为**策略迭代**。 策略迭代的过程如下：

*   从随机策略`π[0]`开始
*   对于迭代步骤`t`中给定的`π[t]`策略，请使用以下公式计算`U[t] = U[t]^π`：

    ![](img/41e3cc56-1bb4-4874-b6db-8246f4054d3e.png)

*   根据以下公式改进`π[t+1]`：

    ![](img/1c9d9376-7286-498e-b097-c6aa03197556.png)

# 部分可观察的马尔可夫决策过程

在 MDP 中，可观察的数量是动作，设置`A`，状态，设置`S`，转移模型，`T`和奖励，设置`R`。 在**部分可观察的 MDP**（也称为 **POMDP**）的情况下，情况并非如此。 在 POMDP 中，内部存在一个 MDP，代理无法直接观察到它，而是根据所做的任何观察来做出决定。

在 POMDP 中，有一个观测集`Z`，它包含不同的可观测状态和观测函数`O`，它采用`s`状态和`z`观察值作为输入，输出在`s`状态下看到`z`观察值的可能性。

POMDP 基本上是 MDP 的概括：

*   **MDP**：`{S, A, T, R}`

*   **POMDP**：`{S, A, Z, T, R, O}`

*   其中，`S`，`A`，`T`和`R`相同。 因此，要使 POMDP 成为真正的 MDP，请满足以下条件：

![](img/305943b7-4263-4be1-aa15-dc23670ece53.png)，即完全遵守所有状态

![](img/b1be7b8a-48e3-400e-96ed-548c592e268e.png)

POMDP 难以解决，无法实现最佳解决方案。

# 状态估计

如果我们扩展状态空间，这将有助于我们将 POMDP 转换为 MDP，其中`Z`包含完全可观察的状态空间。 这给出了**信念状态**`b(s)`的概念，这是决策者将在 POMDP 上下文中使用的状态。 置信状态，即`b(s)`给出了代理处于`s`状态的可能性。 因此，置信状态`b`，是代表所有状态上概率分布的向量。 因此，一旦采取行动，信念状态就会更新。

假设存在一个信念状态`b`，该代理采取行动`a`并收到了一些观察结果`z`。 这形成了一个新的信念状态。 因此，我们正在将 POMDP 转换为信念 MDP，其中它将由信念状态组成为 MDP 状态。

根据前述条件，给出的信息是置信状态`b`，动作`a`和观察值`z`。 因此，

`b'(s')`为在`b`，`a`，`z`之后给出的处于`s`状态的概率 ，即`P(s' | b, a, z)`。

![](img/35b5669e-32a7-4c9f-ad01-833abdb38255.png)

其中`p(z | b, a, s') = O(s', z)`和`p(s' | s, b, a) = T(s, a, s')`。 从而，

![](img/628f5062-7f82-4c25-b541-b1e76ae81d96.png)

# POMDP 中的值迭代

POMDP 中的值迭代基本上是从信念 MDP 获得的无限状态空间上的值迭代。

在`t = 0`时，`V[0](b) = 0`

在`t > 0`时，

![](img/d9d073a1-0957-434c-962a-fc558a1bec1c.png)

其中`b'`为`b'(s') = p(s' | b, a, z)`，即`(b, a, z)`和`R(b, a)`的状态估计是对信念状态的预期奖励，如下所示：

![](img/76b35625-61ca-47bf-b1f7-a0b13e381607.png)

哪里，

`p(s)`为`s`状态的概率

`R(s, a)`为该状态下的奖励

![](img/c113e9fc-da9e-4c29-a01a-ae92779162cf.png) =对信念状态的预期奖励

# 使用 MDP 训练 FrozenLake-v0 环境

这是关于 OpenAI Gym 中名为 **FrozenLake-v0** 的网格世界环境，在第 2 章“使用 OpenAI Gym 训练强化学习智能体”中讨论。 我们实现了 Q 学习和 Q 网络（我们将在以后的章节中进行讨论）以了解 OpenAI Gym 的环境。

现在，让我们尝试使用以下代码实现值迭代，以获取 FrozenLake-v0 环境中每个状态的效用值：

```py
# importing dependency libraries
from __future__ import print_function
import gym
import numpy as np
import time

#Load the environment
env = gym.make('FrozenLake-v0')

s = env.reset()
print(s)
print()

env.render()
print()

print(env.action_space) #number of actions
print(env.observation_space) #number of states
print()

print("Number of actions : ",env.action_space.n)
print("Number of states : ",env.observation_space.n)
print()

# Value Iteration Implementation

#Initializing Utilities of all states with zeros
U = np.zeros([env.observation_space.n])

#since terminal states have utility values equal to their reward
U[15] = 1 #goal state
U[[5,7,11,12]] = -1 #hole states
termS = [5,7,11,12,15] #terminal states
#set hyperparameters
y = 0.8 #discount factor lambda

eps = 1e-3 #threshold if the learning difference i.e. prev_u - U goes below this value break the learning

i=0
while(True):
    i+=1
    prev_u = np.copy(U)
    for s in range(env.observation_space.n):
        q_sa = [sum([p*(r + y*prev_u[s_]) for p, s_, r, _ in env.env.P[s][a]]) for a in range(env.action_space.n)]
        if s not in termS: 
            U[s] = max(q_sa)
    if (np.sum(np.fabs(prev_u - U)) <= eps):
        print ('Value-iteration converged at iteration# %d.' %(i+1))
        break

print("After learning completion printing the utilities for each states below from state ids 0-15")
print()
print(U[:4])
print(U[4:8])
print(U[8:12])
print(U[12:16])
------------------------------------------------------------------------------------------------
<<OUTPUT>>
[2018-04-16 20:59:03,661] Making new env: FrozenLake-v0
0

SFFF
FHFH
FFFH
HFFG

Discrete(4)
Discrete(16)

Number of actions : 4
Number of states : 16

Value-iteration converged at iteration# 25.
After learning completion printing the utilities for each states below from state ids 0-15

[ 0.023482 0.00999637 0.00437564 0.0023448 ]
[ 0.0415207 -1\. -0.19524141 -1\. ]
[ 0.09109598 0.20932556 0.26362693 -1\. ]
[-1\. 0.43048408 0.97468581 1\. ]
```

分析输出，

让状态表示如下：

0 1 2 3

4 5 6 7

8 9 10 11

12 13 14 15

我们代理的起始状态为 0。让我们从`s = 0`开始，

`U[s = 0] = 0.023482`，现在动作可以是`UP`，`DOWN`，`LEFT`或`RIGHT` 。

在`s = 0`的情况下，如果：

*   采取`UP`动作，`s_new = o`，因此，`u[s_new] = 0.023482`
*   采取`DOWN`动作，`s_new = 4`，因此，`u[s_new] = 0.0415207`
*   采取`LEFT`动作，`s_new = o`，因此，`u[s_new] = 0.023482`
*   采取`RIGHT`动作，`s_new = 1`，因此，`u[s_new] = 0.00999637`

最大值为`u[s_new = 4] = 0.0415207`，因此，采取的动作为`DOWN`和`s_new = 4`。

现在在`s = 4`的情况下：

*   采取`UP`动作，`s_new = o`，因此，`u[s_new] = 0.023482`
*   采取`DOWN`动作，`s_new = 8`，因此，`u[s_new] = 0.09109598`
*   采取`LEFT`动作，`s_new = 4`，因此，`u[s_new] = 0.0415207`
*   采取`RIGHT`动作，`s_new = 5`，因此，`u[s_new] = -1.0`

最大值为`u[s_new = 8] = 0.09109598`，则采取的动作将为`DOWN`和`s_new = 8`。

现在，如果`s = 8`，则：

*   采取`UP`动作，`s_new = 4`，因此，`u[s_new] = 0.0415207`
*   采取`DOWN`动作，`s_new = 12`，因此，`u[s_new] = -1.0`
*   采取`LEFT`动作，`s_new = 8`，因此，`u[s_new] = 0.09109598`
*   采取`RIGHT`动作，`s_new = 9`，因此，`u[s_new] = 0.20932556`

最大值为`u[s_new = 9] = 0.20932556`，因此，采取的动作为`RIGHT`和`s_new = 9`。

现在，如果`s = 9`，则：

*   采取`UP`动作，`s_new = 5`，因此，`u[s_new] = -1.0`
*   采取`DOWN`动作，`s_new = 13`，因此，`u[s_new] = 0.43048408`
*   采取`LEFT`动作，`s_new = 8`，因此，`u[s_new] = 0.09109598`
*   采取`RIGHT`动作，`s_new = 10`，因此，`u[s_new] = 0.26362693`

最大值为`u[s_new = 13] = 0.43048408`，因此，采取的动作为`DOWN`和`s_new = 13`。

现在，如果`s = 13`，则：

*   采取`UP`动作，`s_new = 9`，因此，`u[s_new] = 0.20932556`
*   采取`DOWN`动作，则`s_new = 13`，因此，`u[s_new] = 0.43048408`
*   采取`LEFT`动作，`s_new = 12`，因此，`u[s_new] = -1.0`
*   采取`RIGHT`动作，`s_new = 14`，因此，`u[s_new] = 0.97468581`

最大值为`u[s_new = 14] = 0.97468581`，因此，采取的动作为`RIGHT`和`s_new = 14`。

现在，如果`s = 14`，则：

*   采取`UP`动作，`s_new = 10`，因此，`u[s_new] = 0.26362693`
*   采取`DOWN`动作，`s_new = 14`，因此，`u[s_new] = 0.97468581`
*   采取`LEFT`动作，`s_new = 13`，因此，`u[s_new] = 0.43048408`
*   采取`RIGHT`动作，`s_new = 15`（目标状态），因此，`u[s_new] = 1.0`

最大值为`u [s_new = 15] = 1.0`，因此，采取的动作为`RIGHT`和`s_new = 15`。

因此，我们的策略包含`DOWN`，`DOWN`，`RIGHT`，`DOWN`，`RIGHT`和`RIGHT`通过避开空穴状态（5、7、11、12）从`s = 0`（开始状态）到达`s = 15`（目标状态）。

# 总结

在本章中，我们介绍了网格世界类型的环境的详细信息，并了解了马尔可夫决策过程的基础，即状态，动作，奖励，转移模型和策略。 此外，我们利用这些信息通过价值迭代和策略迭代方法来计算效用和最优策略。

除此之外，我们对部分可观察的马尔可夫决策过程是什么样子以及解决它们的挑战有了基本的了解。 最后，我们从 OpenAI Gym 获取了我们最喜欢的 gridworld 环境，即 FrozenLake-v0，并实现了一种值迭代方法，以使我们的代理学会在该环境中导航。

在下一章中，我们将从策略梯度开始，然后从 **FrozenLake** 过渡到其他一些引人入胜的复杂环境。